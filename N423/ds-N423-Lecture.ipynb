{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ds-cs-N423-Lecture.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"U4-S2-NNF-DS10","language":"python","name":"u4-s2-nnf-ds10"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc-autonumbering":false,"toc-showmarkdowntxt":false},"cells":[{"cell_type":"markdown","metadata":{"id":"dQI1J5xE_6f0"},"source":["<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n","\n","## *DATA SCIENCE / SECTION 4 / SPRINT 2 / NOTE 3*\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"zd1XsIx_kpJl"},"source":["## Warm up\n","\n","다음 영상을 시청해 주세요 :)\n","\n","- [OpenCV를 이용한 MNIST 인식 모델 만들어보기](https://www.youtube.com/watch?v=TV3oplqa5VA) + 더보기 (코드 링크 있음)\n","- [Tensorflow를 이용한 CNN 실습영상](https://www.youtube.com/watch?v=pZGvMhhawy8) + 더보기 (코드 링크 있음)"]},{"cell_type":"markdown","metadata":{"id":"05lYoZsnhriu"},"source":["# 지난시간 복습\n","- 신경망의 동작원리 (Note1)\n","  * 데이터 전처리 및 입력\n","  * 모델 제작 및 가중치 초기화\n","  * 모델에 데이터를 넣고 출력값을 얻음\n","  * 출력값과 레이블(정답지)과 비교 후 Loss 계산\n","  * Loss를 반영하여 가중치 업데이트 -> 역전파(BackPropagation) + 경사하강법(Gradient Descent)\n","- 역전파 원리 및 실습\n","  * Loss function의 계산방식\n","  * Stochastic Gradient Descent 방법\n","  * 경사하강법의 변형들(Adam)\n","  * 2x2x2 neural network의 역전파 수학식\n","- Fashion MNIST 실습\n"]},{"cell_type":"markdown","metadata":{"id":"pifxn0QbJ1Y8"},"source":["# N423. 신경망 구현을 위한 프레임워크 - Tensorflow, Keras"]},{"cell_type":"markdown","metadata":{"id":"GTeYAMysJ1Y9"},"source":["## 학습목표\n","* <a href=\"#p1\">Part 1</a>: 모델 아키텍쳐를 어떻게 선택하는 지 배우게 됩니다.\n","* <a href=\"#p3\">Part 2</a>: 가중치의 규제(Regularization) 전략을 배웁니다. \n","* <a href=\"#p2\">Part 3</a>: 다양한 활성함수를 사용함에 발생하는 trade-off에 대해서 논의해볼 수 있어야 합니다. \n","\n","\n","## Let's Use Libraries !\n","\n","지난 이틀간의 목표는 신경망의 배경, 기초, 용어, 네트워크 구조, 전파/역전파, 오류/비용 함수, 에폭(Epoch), 그리고 경사하강법 등을 숙지하는 것이었죠. Perceptrons(단일 노드 신경망)와 Feed-Forward Neural Networks라고도 알려진 Multi-Layer Perceptrons를 포함하여 간단한 신경망을 손으로 코딩하도록 요구하여 함으로써 신경망에 익숙해지기 위해 노력해왔죠. \n","\n","수작업으로 한땀 한땀 신경망을 만드는 것은 우리의 시간을 사용하는 최선의 방법은 아닐 것이라는 것을 아실 것이고, 이제는 보다 편하게 모듈화된 자료들을 하나씩 배워볼 수 있습니다. 실무에서 사용할 예측 모델을 만들기 위해 강력한 라이브러리를 사용하기 시작할 것입니다. Let's Go!"]},{"cell_type":"markdown","metadata":{"id":"xpjX2C8uJ1Y_"},"source":["## 개요\n","\n","> 딥러닝 연구자들의 일부는 신경망을 위한 아키텍쳐(구조)을 선택하는 것은 과학이라기 보다는 예술에 가깝다고 말합니다. \n","\n","> 한편, 노가다라고 말하기도합니다. 0.1%를 올리기 위해서 수많은 작업들이 진행되기도 하니까요.\n","\n","> 용도에 맞는 구조를 선택하는 가장 좋은 방법은 연구와 실험을 통해서 발견할 수 있기 때문입니다."]},{"cell_type":"code","metadata":{"id":"Y9BMoaFJMv6-"},"source":["# 파일 선택을 통해 예제 데이터를 내 컴퓨터에서 불러오는 코드를 포함(주석)\n","# 강의 목적상 내 데이터를 대신하여 서버에서 불러오도록 하겠습니다. 직접 가지고 있는 데이터를 사용하기 위해서는 주석처리된 files.upload()를 이용하시면 됩니다.\n","from google.colab import files\n","#uploaded = files.upload() # 파일을 불러올 수 있는 코드\n","\n","my_data = \"https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/everydeep/ThoraricSurgery.csv\"\n","\n","# 딥러닝을 구동하는 데 필요한 케라스 함수를 불러옵니다.\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","import tensorflow.keras.layers as Layer\n","\n","# 케라스 외의 필요한 라이브러리를 불러옵니다.\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","# 실행할 때마다 같은 결과를 출력하기 랜덤함수를 고정하는 부분입니다.\n","# 랜덤함수의 Seed를 고정하게 되면 랜덤함수가 항상 일정하게 나옵니다. \n","np.random.seed(3)\n","tf.random.set_seed(3)\n","\n","# 불러온 데이터를 적용합니다.\n","# pandas외에도 읽을 수 있는 방법이 있습니다. 편하신 방법을 사용하시면 됩니다.\n","Data_set = np.loadtxt(my_data, delimiter=\",\") "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rx-kY0-l-DXB","colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"status":"ok","timestamp":1605579133312,"user_tz":-540,"elapsed":1882,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"c57e303d-a056-4364-abf9-d5a5c8f9955d"},"source":["Data_set = pd.read_csv(my_data, header=None) \n","Data_set.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","      <td>470.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>235.500000</td>\n","      <td>3.095745</td>\n","      <td>3.281638</td>\n","      <td>4.568702</td>\n","      <td>0.780851</td>\n","      <td>0.065957</td>\n","      <td>0.144681</td>\n","      <td>0.065957</td>\n","      <td>0.687234</td>\n","      <td>0.165957</td>\n","      <td>11.736170</td>\n","      <td>0.074468</td>\n","      <td>0.004255</td>\n","      <td>0.017021</td>\n","      <td>0.821277</td>\n","      <td>0.004255</td>\n","      <td>62.534043</td>\n","      <td>0.148936</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>135.821574</td>\n","      <td>0.722309</td>\n","      <td>0.871395</td>\n","      <td>11.767857</td>\n","      <td>0.535375</td>\n","      <td>0.248472</td>\n","      <td>0.352154</td>\n","      <td>0.248472</td>\n","      <td>0.464114</td>\n","      <td>0.372439</td>\n","      <td>0.702243</td>\n","      <td>0.262811</td>\n","      <td>0.065163</td>\n","      <td>0.129488</td>\n","      <td>0.383529</td>\n","      <td>0.065163</td>\n","      <td>8.706902</td>\n","      <td>0.356405</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.440000</td>\n","      <td>0.960000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>11.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>21.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>118.250000</td>\n","      <td>3.000000</td>\n","      <td>2.600000</td>\n","      <td>1.960000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>11.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>57.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>235.500000</td>\n","      <td>3.000000</td>\n","      <td>3.160000</td>\n","      <td>2.400000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>12.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>62.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>352.750000</td>\n","      <td>3.000000</td>\n","      <td>3.807500</td>\n","      <td>3.080000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>12.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>69.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>470.000000</td>\n","      <td>8.000000</td>\n","      <td>6.300000</td>\n","      <td>86.300000</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>14.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>87.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               0           1           2   ...          15          16          17\n","count  470.000000  470.000000  470.000000  ...  470.000000  470.000000  470.000000\n","mean   235.500000    3.095745    3.281638  ...    0.004255   62.534043    0.148936\n","std    135.821574    0.722309    0.871395  ...    0.065163    8.706902    0.356405\n","min      1.000000    1.000000    1.440000  ...    0.000000   21.000000    0.000000\n","25%    118.250000    3.000000    2.600000  ...    0.000000   57.000000    0.000000\n","50%    235.500000    3.000000    3.160000  ...    0.000000   62.000000    0.000000\n","75%    352.750000    3.000000    3.807500  ...    0.000000   69.000000    0.000000\n","max    470.000000    8.000000    6.300000  ...    1.000000   87.000000    1.000000\n","\n","[8 rows x 18 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"4i71RFpt9_aH"},"source":["Data_set = np.loadtxt(my_data, delimiter=\",\") \n","# 환자의 기록과 수술 결과를 X와 Y로 구분하여 저장합니다.\n","X = Data_set[:,0:17]\n","Y = Data_set[:,17]\n","\n","# 딥러닝 구조를 결정합니다(모델을 설정하고 실행하는 부분입니다).\n","model = Sequential([\n","    Dense(30, input_dim=17, activation='relu'),\n","    Layer.Dropout(0.5),\n","    Dense(30, input_dim=17, activation='relu'),\n","    Dense(1, activation='sigmoid') # 분류할 방법에 따라 개수를 조정해야 합니다. \n","])\n","# 딥러닝을 실행합니다.\n","model.compile(loss='mean_squared_logarithmic_error', optimizer='adam', metrics=['accuracy']) # mean_squared_error # binary_crossentropy # mean_absolute_error # poisson\n","history = model.fit(X, Y, epochs=30, batch_size=30)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOPZTmXpbsmO"},"source":["## 오차함수\n","\n","평균제곱계열\n","- (복습) mean_squared_error (MSE) = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}$\n","- (복습) RMSE (Root Mean Squared Error) = \n","$\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}}$\n","- (복습) mean_absolute_error (MAE) = $\\frac{1}{n}\\sum_{i=1}^{n}\\left | y_{i} - \\hat{y_{i}} \\right |$\n","- (복습) R-Squared (coefficient of determination) = $1 - \\frac{\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}}{\\sum_{i=1}^{n}(y_{i} - \\bar{y_{i}})^{2}} = 1 - \\frac{SSE}{SST} = \\frac {SSR}{SST}$\n","  -  SSE, SST, SSR: Sum of Squared `Error`, `Total`, `Regression`($\\sum_{i=1}^{n}(\\hat{y_{i}} - \\bar{y_{i}})^{2}$)\n","- mean_absolute_percentage_error = $ \\frac {1}{n}\\sum _{i=1}^{n}\\left|{\\frac {y_{t}-\\hat{y_{i}}}{y_{i}}}\\right| $\n","- mean_squared_logarithmic_error = $\\frac{1}{n} \\sum_{i=1}^n (\\log(\\hat{y_i} + 1) - \\log(y_i+1))^2 $\n","\n","\n","\n","엔트로피계열\n","- binary_crossentropy = $ -\\sum_{c=1}^{C} q(y_c) log(q(y_c)), \\hspace{2em} q(y_c) \\in (1, -1)$\n","- categorical_crossentropy = $ -\\sum_{c=1}^{C} q(y_c)log(q(y_c)) $\n","\n","\n","[기타 다른 계열](https://keras.io/api/losses/)"]},{"cell_type":"markdown","metadata":{"id":"TWuoXZCCKCI7"},"source":["# 학습 규제 전략 (Regularization Strategies)"]},{"cell_type":"markdown","metadata":{"id":"3Ylr63BpZ0sj"},"source":["## Fashion MNIST 예제를 통한 활용"]},{"cell_type":"markdown","metadata":{"id":"8OQH1s-vJ1Zl"},"source":["### Overfitting 극복을 위한 노력\n","\n","Neural Networks는 매개변수가 아주 많은 모델이어서, Section 2에서 공부했던 것처럼 훈련 데이터에 쉽게 과대적합(overfit) 오버핏될 수 있다. 이 문제를 해결하는 가장 중요한 방법은 가중치 규제 전략이다.\n","\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Regularization.svg/1920px-Regularization.svg.png\" width = 600 />\n","\n","우리가 간단히 다루는 신경망에는 네 가지 일반적인 가중치 규제 방법이 있다. 이러한 구성 요소를 적용하는 방법:\n","\n","1. 항상 EarlyStopping을 사용한다. 이 전략은 당신의 가중치의 최고 유용성 시점을 훨씬 지나서 더 업데이트되는 것을 막을 것이다.\n","2. EarlyStopping, 가중치 감소(Weight Decay) 및 Dropout 사용\n","3. EarlyStopping, 가중치 제약(Constraint) 및 Dropout 사용\n","\n","Weight Decusion and Weight Restriction은 유사한 목적을 위하여 가중치를 제거하거나 값을 규제하여 매개변수를 과도하게 적합시키는 것을 방지하는 역할이다. 같은 목적의 다른 방법이기 때문에 이들을  굳이 같이 적용하지 않아도 된다. "]},{"cell_type":"code","metadata":{"id":"4N0ie96jMwAs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605517534615,"user_tz":-540,"elapsed":7694,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"906aac8c-0414-414a-f7e2-57c0e745089b"},"source":["# Tensorflow에서 데이터를 가져와 규제 하는 코드\n","\n","from tensorflow.keras.datasets import fashion_mnist\n","\n","# 데이터 불러오기\n","(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n","print(X_train.shape, X_test.shape)\n","\n","# 데이터를 정규화 합니다\n","X_train = X_train / 255.\n","X_test = X_test /255.\n","\n","# 클래스를 확인합니다.\n","np.unique(y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n","(60000, 28, 28) (10000, 28, 28)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"yCCwnpIEMwDh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605517534615,"user_tz":-540,"elapsed":7688,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"8157f254-5ff6-4458-f43e-e1274730e5ef"},"source":["# 기본적인 신경망을 만드는 코드\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten\n","import keras, os\n","\n","# 모델 구성을 확인합니다.\n","model = Sequential([\n","    Flatten(input_shape=(28, 28)),\n","    Dense(10, activation='softmax')\n","])\n","# 업데이트 방식을 설정합니다.\n","model.compile(optimizer='adam'\n","             , loss='sparse_categorical_crossentropy'\n","             , metrics=['accuracy'])\n","model.summary()\n","# 총 7850 parameters (10 bias)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten (Flatten)            (None, 784)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 10)                7850      \n","=================================================================\n","Total params: 7,850\n","Trainable params: 7,850\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2fyjs2pdMwGh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605517869472,"user_tz":-540,"elapsed":32214,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"ed93e10a-2498-4077-a32e-b8a8aacbd889"},"source":["# 모델 학습을 위한 코드\n","\n","# 변수 설정을 따로 하는 방법을 적용하기 위한 코드입니다. \n","batch_size = 30\n","epochs_max = 1\n","\n","# 학습시킨 데이터를 저장시키기 위한 코드입니다. \n","checkpoint_filepath = \"FMbest.hdf5\"\n","\n","# overfitting을 방지하기 위해서 학습 중 early stop을 수행하기 위한 코드입니다.\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n","\n","# Validation Set을 기준으로 가장 최적의 모델을 찾기 위한 코드입니다.\n","save_best = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n","    save_weights_only=True, mode='auto', save_freq='epoch', options=None)\n","\n","# 모델 학습 코드 + early stop + Best model\n","model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs_max, verbose=1, \n","          validation_data=(X_test,y_test), \n","          callbacks=[early_stop, save_best])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","1971/2000 [============================>.] - ETA: 0s - loss: 0.3663 - accuracy: 0.8704\n","Epoch 00001: val_loss improved from inf to 0.45542, saving model to FMbest.hdf5\n","2000/2000 [==============================] - 3s 2ms/step - loss: 0.3664 - accuracy: 0.8705 - val_loss: 0.4554 - val_accuracy: 0.8451\n","Epoch 2/10\n","1974/2000 [============================>.] - ETA: 0s - loss: 0.3677 - accuracy: 0.8705\n","Epoch 00002: val_loss did not improve from 0.45542\n","2000/2000 [==============================] - 3s 2ms/step - loss: 0.3678 - accuracy: 0.8703 - val_loss: 0.4693 - val_accuracy: 0.8349\n","Epoch 3/10\n","1985/2000 [============================>.] - ETA: 0s - loss: 0.3660 - accuracy: 0.8707\n","Epoch 00003: val_loss did not improve from 0.45542\n","2000/2000 [==============================] - 3s 2ms/step - loss: 0.3663 - accuracy: 0.8706 - val_loss: 0.4565 - val_accuracy: 0.8403\n","Epoch 4/10\n","1984/2000 [============================>.] - ETA: 0s - loss: 0.3662 - accuracy: 0.8710\n","Epoch 00004: val_loss improved from 0.45542 to 0.45539, saving model to FMbest.hdf5\n","2000/2000 [==============================] - 3s 2ms/step - loss: 0.3661 - accuracy: 0.8709 - val_loss: 0.4554 - val_accuracy: 0.8439\n","Epoch 5/10\n","1973/2000 [============================>.] - ETA: 0s - loss: 0.3648 - accuracy: 0.8712\n","Epoch 00005: val_loss improved from 0.45539 to 0.45146, saving model to FMbest.hdf5\n","2000/2000 [==============================] - 3s 1ms/step - loss: 0.3649 - accuracy: 0.8711 - val_loss: 0.4515 - val_accuracy: 0.8449\n","Epoch 6/10\n","1998/2000 [============================>.] - ETA: 0s - loss: 0.3648 - accuracy: 0.8709\n","Epoch 00006: val_loss did not improve from 0.45146\n","2000/2000 [==============================] - 3s 2ms/step - loss: 0.3651 - accuracy: 0.8708 - val_loss: 0.4538 - val_accuracy: 0.8414\n","Epoch 7/10\n","1979/2000 [============================>.] - ETA: 0s - loss: 0.3650 - accuracy: 0.8720\n","Epoch 00007: val_loss did not improve from 0.45146\n","2000/2000 [==============================] - 3s 2ms/step - loss: 0.3650 - accuracy: 0.8719 - val_loss: 0.4541 - val_accuracy: 0.8454\n","Epoch 8/10\n","1983/2000 [============================>.] - ETA: 0s - loss: 0.3642 - accuracy: 0.8714\n","Epoch 00008: val_loss improved from 0.45146 to 0.44674, saving model to FMbest.hdf5\n","2000/2000 [==============================] - 3s 2ms/step - loss: 0.3645 - accuracy: 0.8713 - val_loss: 0.4467 - val_accuracy: 0.8438\n","Epoch 9/10\n","1986/2000 [============================>.] - ETA: 0s - loss: 0.3641 - accuracy: 0.8725\n","Epoch 00009: val_loss did not improve from 0.44674\n","2000/2000 [==============================] - 3s 2ms/step - loss: 0.3646 - accuracy: 0.8723 - val_loss: 0.4555 - val_accuracy: 0.8438\n","Epoch 10/10\n","1977/2000 [============================>.] - ETA: 0s - loss: 0.3640 - accuracy: 0.8707\n","Epoch 00010: val_loss did not improve from 0.44674\n","2000/2000 [==============================] - 3s 2ms/step - loss: 0.3637 - accuracy: 0.8709 - val_loss: 0.4591 - val_accuracy: 0.8382\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fc77b5d6ac8>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"8ahbL_VrMwL_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605517892908,"user_tz":-540,"elapsed":934,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"13273436-95fa-4557-c450-624f3adaf206"},"source":["# 학습된 모델을 이용하여 테스트하는 코드\n","\n","model.predict(X_test[0:1])\n","test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["313/313 - 0s - loss: 0.4591 - accuracy: 0.8382\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G2ae-FVgULun","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605517893405,"user_tz":-540,"elapsed":1424,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"772bcd55-d7b2-4811-ff89-988bb5f49eed"},"source":["!ls "],"execution_count":null,"outputs":[{"output_type":"stream","text":["ds-lecture-data.s3.ap-northeast-2.amazonaws.com  FMbest.hdf5  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sOPBbgyNbchL"},"source":["# 체크포인트에 저장된 가중치들을 불러들이는 코드\n","\n","model.load_weights(checkpoint_filepath)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5HBsKMMgbcjn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605517893907,"user_tz":-540,"elapsed":1905,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"1b66c071-5595-49b8-b5a9-133137997f11"},"source":["# best model을 이용한 테스트 데이터 예측 정확도 재확인 코드\n","\n","model.predict(X_test[0:1])\n","test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["313/313 - 0s - loss: 0.4467 - accuracy: 0.8438\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eQYqXSsIiN-3"},"source":["### Weight Decay"]},{"cell_type":"markdown","metadata":{"id":"lNz8nesKIRB_"},"source":["이번에는 학습과정에서 Overfitting을 방지하기 위한 기술, Regularization의 한 종류인 Weight Decay를 수행해보겠습니다. 말 그대로 가중치를 감소시키는 기술입니다. 아래 그림을 다시한번 보면, 굽이치는 그래프를 나타내려면 큰 가중치가 필요합니다. 애초에 큰 가중치를 갖지 못하게 만들면, 다음과 같이 과대적합이 될 수가 없도록 만드는 기술입니다.\n","\n","<img src=\"https://miro.medium.com/max/1400/0*CmDTGlQyibHUORQ0.png\"/>"]},{"cell_type":"markdown","metadata":{"id":"fD-fNWM9Uzgz"},"source":["# keras에서 Weight Decay를 구현하는 방법\n","```\n","Dense(64, input_dim=64,\n","            kernel_regularizer=regularizers.l2(0.01),\n","            activity_regularizer=regularizers.l1(0.01))\n","```"]},{"cell_type":"code","metadata":{"id":"W3jwvMHSiNKT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605518003665,"user_tz":-540,"elapsed":5598,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"daa1d9bd-db9e-4f12-94f5-992b202e0920"},"source":["# Weight Decay를 전체적으로 반영한 예시 코드\n","from tensorflow.keras.constraints import MaxNorm\n","from tensorflow.keras import regularizers\n","\n","# 모델 구성을 확인합니다.\n","model = Sequential([\n","    Flatten(input_shape=(28, 28)),\n","    Dense(64, input_dim=64,\n","            kernel_regularizer=regularizers.l2(0.01),    # L2 norm regularization\n","            activity_regularizer=regularizers.l1(0.01)), # L1 norm regularization\n","    Dense(10, activation='softmax')\n","])\n","# 업데이트 방식을 설정합니다.\n","model.compile(optimizer='adam'\n","             , loss='sparse_categorical_crossentropy'\n","             , metrics=['accuracy'])\n","model.summary()\n","\n","model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, \n","          validation_data=(X_test,y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_4 (Flatten)          (None, 784)               0         \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 64)                50240     \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 10)                650       \n","=================================================================\n","Total params: 50,890\n","Trainable params: 50,890\n","Non-trainable params: 0\n","_________________________________________________________________\n","2000/2000 [==============================] - 4s 2ms/step - loss: 1.0288 - accuracy: 0.7957 - val_loss: 0.7947 - val_accuracy: 0.8092\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fc77965eeb8>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"iOfZojJxita5"},"source":["### Constraints\n","\n","[참고자료](https://keras.io/api/layers/constraints/)"]},{"cell_type":"markdown","metadata":{"id":"-vlTGJMQJRXh"},"source":["Weight Decay를 통해서 Weight의 학습 반경을 변경시켰다면, 이번에는 물리적으로 Weight의 크기를 제한하는 방법입니다. Weight자체를 함수를 이용하여 더 큰 경우는 임의의 값으로 변경해버리는 기술을 사용하게 됩니다. 그러면 더이상 커질 수가 없겠죠? "]},{"cell_type":"code","metadata":{"id":"BK5Uuh3XitK6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605518016205,"user_tz":-540,"elapsed":6795,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"e74911ed-5149-4bc4-c587-0120183a4819"},"source":["# 모델 구성을 확인합니다.\n","model = Sequential([\n","    Flatten(input_shape=(28, 28)),\n","    Dense(64, input_dim=64,\n","            kernel_regularizer=regularizers.l2(0.01),\n","            activity_regularizer=regularizers.l1(0.01),\n","            kernel_constraint=MaxNorm(2.)),             ## add constraints\n","    Dense(10, activation='softmax')\n","])\n","# 업데이트 방식을 설정합니다.\n","model.compile(optimizer='adam'\n","             , loss='sparse_categorical_crossentropy'\n","             , metrics=['accuracy'])\n","model.summary()\n","\n","\n","model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, \n","          validation_data=(X_test,y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_5 (Flatten)          (None, 784)               0         \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 64)                50240     \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 10)                650       \n","=================================================================\n","Total params: 50,890\n","Trainable params: 50,890\n","Non-trainable params: 0\n","_________________________________________________________________\n","2000/2000 [==============================] - 5s 3ms/step - loss: 1.0208 - accuracy: 0.7972 - val_loss: 0.7903 - val_accuracy: 0.8103\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fc77952d7f0>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"ROWAARpPJ1Zt"},"source":["### Dropout"]},{"cell_type":"markdown","metadata":{"id":"rMgvh27GJxgW"},"source":["Dropout의 경우는 위의 경우와는 다른 방식으로 overfitting을 방지합니다. \n","모델 자체에 Layer를 추가하는 방식으로 진행이 되는데요, 이는 확률적으로 노드 연결을 강제로 끊어주는 역할을 합니다. 보톡스를 맞으면 근육을 쓰지 못하게 해서 주름이 생기는 것을 막아버리는데요, 뉴럴넷의 보톡스와 같은 존재라고 할 수 있습니다.  단, 임시로 차단을 하고, 그 연결이 없이 결과를 예측하도록 만들고, 해당 뉴런없이 학습을 진행하기 때문에 과적합을 어느정도 차단할 수 있습니다. "]},{"cell_type":"code","metadata":{"id":"RGJ4Vu1TJ1Zt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605518021820,"user_tz":-540,"elapsed":12405,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"e543f021-bb58-4e39-a849-c3fbcc159cd3"},"source":["from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.constraints import MaxNorm\n","from tensorflow.keras import regularizers\n","# 모델 구성을 확인합니다.\n","model = Sequential([\n","    Flatten(input_shape=(28, 28)),\n","    Dense(64, input_dim=64,\n","            kernel_regularizer=regularizers.l2(0.01),\n","            activity_regularizer=regularizers.l1(0.01),\n","            kernel_constraint=MaxNorm(2.)),             \n","    Dropout(0.5)       ,                                   ## add dropout\n","    Dense(10, activation='softmax')\n","])\n","# 업데이트 방식을 설정합니다.\n","model.compile(optimizer='adam'\n","             , loss='sparse_categorical_crossentropy'\n","             , metrics=['accuracy'])\n","model.summary()\n","\n","\n","model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, \n","          validation_data=(X_test,y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_6 (Flatten)          (None, 784)               0         \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 64)                50240     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 10)                650       \n","=================================================================\n","Total params: 50,890\n","Trainable params: 50,890\n","Non-trainable params: 0\n","_________________________________________________________________\n","2000/2000 [==============================] - 5s 2ms/step - loss: 1.2017 - accuracy: 0.7687 - val_loss: 0.8811 - val_accuracy: 0.8032\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fc7797ddc88>"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"HovvOafCKfvv"},"source":["### 조금 더 나은 모델학습을 위하여!"]},{"cell_type":"markdown","metadata":{"id":"MZHnwSH1KYUI"},"source":["이번에는 조금 다른 기술들에 대해서 배워보겠습니다.  \n","\n","무엇인가 추가하는 방법이 아니라 기존의 방법을 어떻게 활용할 지에 대한 고민입니다. 위의 논문은 실무에 들어가시게 되면 꼭 한번 읽어보세요.\n","\n","CNN에 적용되는 기술로 소개되었지만, 최근에는 그 경계가 사라졌고, 지금 배워도 유용한 기술이기 때문에 미리 배워보겠습니다."]},{"cell_type":"markdown","metadata":{"id":"sQ1sHzQvUv6V"},"source":["학습을 어떻게 효율적으로 할 것인가? \n","\n","[참고논문](https://https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks_CVPR_2019_paper.pdf)\n","Bag of Tricks for Image Classification with Convolutional Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"St-6HRDepXjT"},"source":["### Learning rate decay\n","\n","decay 전략은 다음처럼 활용할 수 있습니다.\n","\n","```\n","tf.keras.optimizers.Adam(\n","    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n","    name='Adam'\n",")\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"ypF6FCdhq4Aw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605518027091,"user_tz":-540,"elapsed":17671,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"37fb399b-bf21-459f-be13-84dfb37ec1d3"},"source":["model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1 = 0.89)\n","             , loss='sparse_categorical_crossentropy'\n","             , metrics=['accuracy'])\n","model.summary()\n","\n","\n","model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, \n","          validation_data=(X_test,y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_6 (Flatten)          (None, 784)               0         \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 64)                50240     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 10)                650       \n","=================================================================\n","Total params: 50,890\n","Trainable params: 50,890\n","Non-trainable params: 0\n","_________________________________________________________________\n","2000/2000 [==============================] - 5s 2ms/step - loss: 0.9220 - accuracy: 0.7905 - val_loss: 0.8472 - val_accuracy: 0.8082\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fc77c62cb38>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"M-ukf8l9tUkj"},"source":["이제 신경망에 조금 익숙해졌다면, 파라미터 개수도 유심히 보면 좋습니다. \n","\n","784 x 64 + 64 = 50240"]},{"cell_type":"markdown","metadata":{"id":"-vYywFfcuIdt"},"source":["#### learning rate 스케쥴링"]},{"cell_type":"markdown","metadata":{"id":"vu7FJ5MOrreA"},"source":["\n","\n","```\n","lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=1e-2,\n","    decay_steps=10000,\n","    decay_rate=0.9)\n","optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"WuHw2hYUrwEE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605518032700,"user_tz":-540,"elapsed":23273,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"d07669d2-e112-4b66-a0b8-8b9784d8c9d4"},"source":["lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=1e-2,\n","    decay_steps=10000,\n","    decay_rate=0.9)\n","\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n","             , loss='sparse_categorical_crossentropy'\n","             , metrics=['accuracy'])\n","model.summary()\n","\n","\n","model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, \n","          validation_data=(X_test,y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_6 (Flatten)          (None, 784)               0         \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 64)                50240     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 10)                650       \n","=================================================================\n","Total params: 50,890\n","Trainable params: 50,890\n","Non-trainable params: 0\n","_________________________________________________________________\n","2000/2000 [==============================] - 5s 2ms/step - loss: 1.9535 - accuracy: 0.6693 - val_loss: 1.8102 - val_accuracy: 0.6893\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fc77c4e7e10>"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"MG47-uF2p7oH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605518038073,"user_tz":-540,"elapsed":28642,"user":{"displayName":"Sibaek Seong","photoUrl":"","userId":"03151236595317567383"}},"outputId":"6401d583-6106-4689-f399-b5c0af50b198"},"source":["def decayed_learning_rate(step):\n","  step = min(step, decay_steps)\n","  cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n","  decayed = (1 - alpha) * cosine_decay + alpha\n","  return initial_learning_rate * decayed\n","\n","first_decay_steps = 1000\n","initial_learning_rate = 0.01\n","lr_decayed_fn = (\n","  tf.keras.experimental.CosineDecayRestarts(\n","      initial_learning_rate,\n","      first_decay_steps))\n","\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n","             , loss='sparse_categorical_crossentropy'\n","             , metrics=['accuracy'])\n","model.summary()\n","\n","\n","model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, \n","          validation_data=(X_test,y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_6 (Flatten)          (None, 784)               0         \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 64)                50240     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 10)                650       \n","=================================================================\n","Total params: 50,890\n","Trainable params: 50,890\n","Non-trainable params: 0\n","_________________________________________________________________\n","2000/2000 [==============================] - 5s 2ms/step - loss: 1.9421 - accuracy: 0.6555 - val_loss: 1.6320 - val_accuracy: 0.7214\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fc77c3b4f60>"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"tcjMuxtn6wIQ","toc-hr-collapsed":false},"source":["# + Activation Functions (Plus 학습)"]},{"cell_type":"markdown","metadata":{"id":"hm6p1HWbEhYi"},"source":["### Tanh Function\n","\n","![Tanh Function](http://mathworld.wolfram.com/images/interactive/TanhReal.gif)\n","\n","만약 sigmoid 함수가 0에서 멀어질 때 바로 그렇게 평평해지지 않고 중간에 조금 더 뾰족해진다면? 그것은 기본적으로 Tanh 기능이다. Tanh 함수는 실제로 y 치수에서 sigmoid 함수를 2로 스케일링하고 모든 값에서 1을 빼면 생성될 수 있다. 기본적으로 sigmoid와 동일한 성질을 가지며, 여전히 우리가 0에서 멀어질수록 평평한 구배 감소에 어려움을 겪고 있지만, 그 파생상품은 0 주위에 더 높아져 가중치가 극단으로 조금 더 빠르게 이동한다."]},{"cell_type":"markdown","metadata":{"id":"XWdvWOBIETwk"},"source":["### Leaky ReLU\n","\n","<img src=\"https://cdn-images-1.medium.com/max/1600/1*ypsvQH7kvtI2BhzR2eT_Sw.png\" width=600/>\n","\n","ReLU가 제일 좋다고만 들었는데, 인성에 문제가 있어? 보통 그래프의 왼쪽 절반(음수)의 함수는 뉴런이 활성화되지 않도록 하는 것을 알고 있죠.  가중치로 초기화된 뉴런의 경우, 우리의 구배는 뉴런의 가중치를 업데이트하지 않을 것이며, 이것은 결코 발화하지 않고 가중치이 업데이트하지 않는 죽은 뉴런, 쓸데없이 메모리를 차지하는 뉴런으로 될 수 있음을 보여준다. 우리는 아마도 초기 가중치가 안 좋게 생성되는 경우를 대비해서 조금이라도 발화하지 않는 뉴런의 가중치를 업데이트하고 미래에 다시 켤 수 있는 기회를 주고 싶을 것이다.\n","\n","Leaky ReLU는 정확히 그것을 해결합니다! 파생 기능 왼쪽(음수)에서 0의 경사를 피함으로 해결합니다. 이는 '죽은' 뉴런도 충분한 반복에 의해 재생될 가능성이 있다는 것을 의미한다. 일부 규격에서는 누출되는 좌측의 기울기를 모델의 하이퍼 파라미터로 실험할 수도 있다!"]},{"cell_type":"markdown","metadata":{"id":"FcAxkNFREMFb"},"source":["### Softmax Function\n","\n","![Softmax Function](https://cdn-images-1.medium.com/max/800/1*670CdxchunD-yAuUWdI7Bw.png)\n","\n","sigmoid 함수와 유사하지만 다중 클래스 분류 문제에 더 유용하다. 소프트맥스 함수는 모든 입력 집합을 취하여 최대 1까지 합한 확률로 변환할 수 있다. 이것은 우리가 어떤 출력물 목록을 던질 수 있다는 것을 의미하며, 그것은 확률로 변환할 것이고, 이것은 다중 클래스 분류 문제에 매우 유용하다. 예를 들어 MNIST처럼..."]},{"cell_type":"markdown","metadata":{"id":"e9n67Z4eks1j"},"source":["# Review\n","* Keras의 예제 튜토리얼을 통해서 나 혼자 노트를 만들어 볼 수 있는 지 확인해본다.\n","* 학습 규제 전략에 대해서 개념을 설명할 수 있다. \n","  - Weight Decay\n","  - Weight constriant\n","  - Drop-Out\n","  - Learning rate \n","* 새롭게 배운 Activation function에 대해서 설명할 수 있다.\n","  - hyperbolic tangent (Tanh)\n","  - Leaky ReLU\n","  - Softmax"]}]}