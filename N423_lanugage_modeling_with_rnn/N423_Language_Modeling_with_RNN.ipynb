{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N423_Language Modeling with RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DcDD5N0WLl5W",
        "mTGgyQtTTiIN",
        "hz6d-H76Pf31",
        "QzDfikzTSE-b",
        "hV7LD6HQSI1D",
        "aT9JUEMYULmX",
        "l5VoE0ohXw2d",
        "svBQyfcmYMLJ",
        "GHoCbrgyYdeE",
        "w9SIBLuVYmjS",
        "ZvuSPMf_ZIji"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_eOv4kAYb0Z"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## ***DATA SCIENCE / SECTION 4 / SPRINT 2 / NOTE 3***\n",
        "\n",
        "---\n",
        "\n",
        "# ì–¸ì–´ ëª¨ë¸ê³¼ RNN(Recurrent Neural Network, ìˆœí™˜ ì‹ ê²½ë§)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSdN5u4fJ6ol"
      },
      "source": [
        "## ğŸ† í•™ìŠµ ëª©í‘œ\n",
        "\n",
        "- ì–¸ì–´ ëª¨ë¸ (Language Model)\n",
        "    - í†µê³„ ê¸°ë°˜ ì–¸ì–´ëª¨ë¸ì„ ì´í•´í•˜ê³  ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    - í†µê³„ ê¸°ë°˜ ì–¸ì–´ëª¨ë¸ì˜ í•œê³„ë¥¼ ì´í•´í•˜ê³  ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë“±ì¥í•œ ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ì˜ ì¥ì ì„ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
        "\n",
        "- ìˆœí™˜ ì‹ ê²½ë§ (Recurrent Neural Network, RNN)\n",
        "    - RNNì˜ êµ¬ì¡°ì™€ ì‘ë™ ë°©ì‹ì„ ì´í•´í•˜ê³  ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    - RNNì˜ ì¥ì ê³¼ ë‹¨ì ì„ ì„¤ëª…í•˜ê³  ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- LSTM & GRU\n",
        "    - LSTMê³¼ GRUê°€ ê³ ì•ˆëœ ë°°ê²½ê³¼ êµ¬ì¡°ë¥¼ ì—°ê´€ì§€ì–´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    - ë‘ ë°©ë²•ì˜ ì°¨ì´ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- Attention\n",
        "    - Attentionì´ íƒ„ìƒí•˜ê²Œ ëœ ë°°ê²½ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    - Attentionì˜ ì¥ì ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ê³  Attention ìœ¼ë¡œë„ í•´ê²°í•  ìˆ˜ ì—†ëŠ” RNNì˜ êµ¬ì¡°ì  ë‹¨ì ì— ëŒ€í•´ì„œë„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAS2HDrvZKE-"
      },
      "source": [
        "## Warm up\n",
        "\n",
        "- [RNN ì†Œê°œ ì˜ìƒ](https://youtu.be/PahF2hZM6cs)\n",
        "\n",
        "- [LSTM ì†Œê°œ ì˜ìƒ](https://youtu.be/bX6GLbpw-A4)\n",
        "\n",
        "- [Seq2Seq êµ¬ì¡°ì™€ Attention ì†Œê°œ ì˜ìƒ](https://youtu.be/WsQLdu2JMgI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcDD5N0WLl5W"
      },
      "source": [
        "## 1. ì–¸ì–´ ëª¨ë¸ (Language Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTGgyQtTTiIN"
      },
      "source": [
        "### 1) ì–¸ì–´ ëª¨ë¸(Language Model)ì´ë€?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZMGEO6aZuze"
      },
      "source": [
        "ì–¸ì–´ ëª¨ë¸ì´ë€ ë¬¸ì¥ê³¼ ê°™ì€ ë‹¨ì–´ ì‹œí€€ìŠ¤ì—ì„œ ê° ë‹¨ì–´ì˜ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.<br/>\n",
        "ì´ì „ ì‹œê°„ì— ë°°ìš´ **`Word2Vec`** ì—­ì‹œ ì—¬ëŸ¬ ê°€ì§€ ì–¸ì–´ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.<br/>\n",
        "**`CBoW`** ì—ì„œëŠ” ì£¼ë³€ ë‹¨ì–´ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íƒ€ê²Ÿ ë‹¨ì–´ì˜ í™•ë¥ ì„ í• ë‹¹í–ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx5Bdjcdd2rA"
      },
      "source": [
        "ìµìˆ™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆê² ì§€ë§Œ ìˆ˜ì‹ìœ¼ë¡œ ë¨¼ì € ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤.<br/>\n",
        "$l$ê°œì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ ë¬¸ì¥ì€ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "> $w_1, w_2, w_3, ..., w_l$\n",
        "\n",
        "`CBoW`ê°€ íƒ€ê²Ÿ ë‹¨ì–´(target word)ë¥¼ ì˜ˆì¸¡í•  í™•ë¥  $P(w_t)$ ì€ ì•„ë˜ì™€ ê°™ì´ êµ¬í•´ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "> $P(w_t \\vert w_{t-2},w_{t-1},w_{t+1},w_{t+2})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ2SV8W8ghGk"
      },
      "source": [
        "`Word2Vec` ì´ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ ë§ì€ ì–¸ì–´ ëª¨ë¸ì€ ëª©í‘œ ë‹¨ì–´ ì™¼ìª½ì˜ ë‹¨ì–´ë§Œì„ ê³ ë ¤í•˜ì—¬ í™•ë¥ ì„ ê³„ì‚°í•˜ì˜€ìŠµë‹ˆë‹¤.<br/>\n",
        "$t$ ë²ˆì§¸ë¡œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ 0ë²ˆì§¸ ë¶€í„° $t-1$ ë²ˆì§¸ ê¹Œì§€ì˜ ëª¨ë“  ë‹¨ì–´ ì •ë³´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì–¸ì–´ ëª¨ë¸ì´ ëª©í‘œ ë‹¨ì–´ ì™¼ìª½ì˜ ë‹¨ì–´ë§Œì„ ê³ ë ¤í•  ë•Œ ë¬¸ì¥ì—ì„œ $t$ ë²ˆì§¸ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  í™•ë¥ ì€ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> $P(w_t \\vert w_{t-1},w_{t-2}, \\cdots ,w_1,w_0)$\n",
        "\n",
        "$l$ ê°œì˜ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì¥ì´ ë§Œë“¤ì–´ì§ˆ í™•ë¥ ì€ ì•„ë˜ ì‹ê³¼ ê°™ì•„ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "> $P(w_0,w_1, \\cdots, w_{l-1}, w_l) = P(w_0)P(w_1 \\vert w_0) \\cdots P(w_{l-1} \\vert w_{l-2}, \\cdots, w_1, w_0)P(w_l \\vert w_{l-1}, w_{l-2}, \\cdots, w_1, w_0)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tWjA2bwWIqy"
      },
      "source": [
        "ìˆ˜ì‹ì´ ìµìˆ™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì˜ˆì‹œë¡œ í•œ ë²ˆ ë” ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ìœ„ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ \"I am a student\" ë¼ëŠ” ë¬¸ì¥ì´ ë§Œë“¤ì–´ì§ˆ í™•ë¥ ì„ êµ¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "> $P(\\text{\"I\",\"am\",\"a\",\"student\"}) = P(\\text{\"I\"}) \\times P(\\text{\"am\"} \\vert \\text{\"I\"}) \\times P(\\text{\"a\"} \\vert \\text{\"I\",\"am\"}) \\times P(\\text{\"student\"} \\vert \\text{\"I\",\"am\",\"a\"})$\n",
        "\n",
        "ì• ë‹¨ì–´ ë“¤ì´ ë“±ì¥í–ˆì„ ë•Œ íŠ¹ì • ë‹¨ì–´ê°€ ë“±ì¥í•  í™•ë¥ ì„ ì¡°ê±´ë¶€ í™•ë¥ ë¡œ êµ¬í•˜ê²Œ ë©ë‹ˆë‹¤.<br/>\n",
        "ì–¸ì–´ ëª¨ë¸ì„ ì˜ ë‚˜íƒ€ë‚´ëŠ” ì˜ìƒ í•˜ë‚˜ ë³´ê³  ê°€ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "72MjG-GF_t4o",
        "outputId": "0adfe000-a2ca-4112-ded1-d0687a90459d"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9GTr4rqlRyw?start=180&end=200\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9GTr4rqlRyw?start=180&end=200\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz6d-H76Pf31"
      },
      "source": [
        "### 2) í†µê³„ì  ì–¸ì–´ ëª¨ë¸ (Statistical Language Model, SLM)\n",
        "\n",
        "í†µê³„ì  ì–¸ì–´ ëª¨ë¸ì€ ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ì´ ì£¼ëª©ë°›ê¸° ì „ë¶€í„° ì—°êµ¬ë˜ì–´ ì˜¨ ì „í†µì ì¸ ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmvDGKrbZy0z"
      },
      "source": [
        "\n",
        "- **í†µê³„ì  ì–¸ì–´ ëª¨ë¸ì˜ í™•ë¥  ê³„ì‚°**\n",
        "\n",
        "í†µê³„ì  ì–¸ì–´ ëª¨ë¸ì—ì„œëŠ” ë‹¨ì–´ì˜ ë“±ì¥ íšŸìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¡°ê±´ë¶€ í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "ë‹¤ì‹œ _\"I am a student\"_ ë¼ëŠ” ë¬¸ì¥ì„ ë§Œë“œëŠ” ì˜ˆì‹œë¥¼ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "> $P(\\text{\"I\",\"am\",\"a\",\"student\"}) = P(\\text{\"I\"}) \\times P(\\text{\"am\"} \\vert \\text{\"I\"}) \\times P(\\text{\"a\"} \\vert \\text{\"I\",\"am\"}) \\times P(\\text{\"student\"} \\vert \\text{\"I\",\"am\",\"a\"})$\n",
        "\n",
        "ì²« ë²ˆì§¸ í•­ì¸ $P(\\text{\"I\"})$ ë¥¼ êµ¬í•´ë´…ì‹œë‹¤. <br/> ì „ì²´ ë§ë­‰ì¹˜ì˜ ë¬¸ì¥ ì¤‘ì—ì„œ ì‹œì‘í•  ë•Œ _\"I\"_ ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì˜ íšŸìˆ˜ë¥¼ êµ¬í•©ë‹ˆë‹¤. ì „ì²´ ë§ë­‰ì¹˜ì˜ ë¬¸ì¥ì´ 1000ê°œì´ê³ , ê·¸ ì¤‘ _\"I\"_ ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì´ 100ê°œë¼ë©´\n",
        "\n",
        "> $$P(\\text{\"I\"}) = \\frac{100}{1000} = \\frac{1}{10}$$\n",
        "\n",
        "ë‹¤ìŒìœ¼ë¡œ, _\"I\"_ ë¡œ ì‹œì‘í•˜ëŠ” 100ê°œì˜ ë¬¸ì¥ ì¤‘ ë°”ë¡œ ë‹¤ìŒì— _\"am\"_ ì´ ë“±ì¥í•˜ëŠ” ë¬¸ì¥ì´ 50ê°œë¼ë©´ \n",
        "\n",
        "> $$P(\\text{\"am\"} \\vert \\text{\"I\"}) = \\frac{50}{100} = \\frac{1}{2}$$\n",
        "\n",
        "ì´ëŸ° ë°©ì‹ìœ¼ë¡œ ëª¨ë“  ì¡°ê±´ë¶€ í™•ë¥ ì„ êµ¬í•œ ë’¤ ì„œë¡œë¥¼ ê³±í•´ì£¼ë©´ ë¬¸ì¥ì´ ë“±ì¥í•  í™•ë¥  $P(\\text{\"I\",\"am\",\"a\",\"student\"})$ ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wjMRaIFfZPw"
      },
      "source": [
        "- **í†µê³„ì  ì–¸ì–´ ëª¨ë¸ì˜ í•œê³„ì **\n",
        "\n",
        "í†µê³„ì  ì–¸ì–´ ëª¨ë¸ì€ íšŸìˆ˜ ê¸°ë°˜ìœ¼ë¡œ í™•ë¥ ì„ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì— í¬ì†Œì„±(Sparsity) ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•™ìŠµì‹œí‚¬ ë§ë­‰ì¹˜ì— _\"1 times\", \"2 times\", ..._ ë¼ëŠ” í‘œí˜„ì€ ë“±ì¥í•˜ì§€ë§Œ _\"7 times\"_ ë¼ëŠ” í‘œí˜„ì€ ì—†ë‹¤ê³  í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ê·¸ë ‡ë‹¤ë©´ ì´ ë§ë­‰ì¹˜ë¥¼ í•™ìŠµí•œ í†µê³„ì  ì–¸ì–´ ëª¨ë¸ì€ ì•„ë˜ì™€ ê°™ì€ ë¬¸ì¥ì„ ì ˆëŒ€ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ì—†ê²Œ ë©ë‹ˆë‹¤.\n",
        "\n",
        "> \"I studied this section 7 times\"\n",
        "\n",
        "_\"7\"_ ì´ë¼ëŠ” ë‹¨ì–´ê°€ ë“±ì¥í•œ ìˆœê°„ ë°”ë¡œ ë‹¤ìŒ _\"times\"_ ê°€ ë“±ì¥í•  í™•ë¥ ì€ 0ì´ ë˜ì–´ë²„ë¦¬ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.<br/>\n",
        "ì´ë ‡ê²Œ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ” í‘œí˜„ì„ì—ë„ ë§ë­‰ì¹˜ì— ë“±ì¥í•˜ì§€ ì•Šì•˜ë‹¤ëŠ” ì´ìœ ë¡œ ë§ì€ ë¬¸ì¥ì´ ë“±ì¥í•˜ì§€ ëª»í•˜ê²Œ ë˜ëŠ” ë¬¸ì œë¥¼ í¬ì†Œ ë¬¸ì œë¼ê³  í•©ë‹ˆë‹¤.<br/>\n",
        "í†µê³„ì  ì–¸ì–´ ëª¨ë¸ì—ì„œ ì´ëŸ° ë¬¸ì œë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ì„œ N-gram ì´ë‚˜ ìŠ¤ë¬´ë”©(smoothing), ë°±ì˜¤í”„(back-off)ì™€ ê°™ì€ ë°©ë²•ì´ ê³ ì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> **ë” ì•Œì•„ë³´ê¸°** <br/>\n",
        "> 1. N-gram : í†µê³„ì  ì–¸ì–´ ëª¨ë¸ì„ ê³ ë„í™” í•˜ê¸° ìœ„í•œ ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ N-gramì— ëŒ€í•´ ì¡°ì‚¬í•´ë´…ì‹œë‹¤.<br/>\n",
        "> 2. Back-off, Smoothing : í¬ì†Œ ë¬¸ì œë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•œ ì¥ì¹˜ì¸ back-off ì™€ smoothingì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzDfikzTSE-b"
      },
      "source": [
        "### 3) ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ (Neural Langauge Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keNrz_89m3HV"
      },
      "source": [
        "ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ì—ì„œëŠ” íšŸìˆ˜ ê¸°ë°˜ ëŒ€ì‹  `Word2Vec`ì´ë‚˜ `fastText` ë“±ì˜ ì¶œë ¥ê°’ì¸ ì„ë² ë”© ë²¡í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. <br/>\n",
        "ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë§ë­‰ì¹˜ì— ë“±ì¥í•˜ì§€ ì•Šë”ë¼ë„ ì˜ë¯¸ì , ë¬¸ë²•ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë¼ë©´ ì„ íƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì„ë² ë”© ë²¡í„°ì—ì„œëŠ” _\"7\"_ ì´ë¼ëŠ” ë‹¨ì–´ì˜ ë²¡í„°ê°€ _\"1\", \"2\" ..._ ë“±ì˜ ë‹¨ì–´ì™€ ìœ ì‚¬í•œ ê³³ì— ìœ„ì¹˜í•©ë‹ˆë‹¤. <br/> ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë§ë­‰ì¹˜ì— _\"7 times\"_ ë¼ëŠ” í‘œí˜„ì´ ë“±ì¥í•˜ì§€ ì•Šë”ë¼ë„ _\"1 times\", \"2 times\"_ ë¼ëŠ” í‘œí˜„ì´ ë“±ì¥í•œë‹¤ë©´ ì–¸ì–´ ëª¨ë¸ì€\n",
        "\n",
        "> \"I studied this section 7 times\"\n",
        "\n",
        "ë¼ëŠ” ë¬¸ì¥ì„ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV7LD6HQSI1D"
      },
      "source": [
        "## 2. ìˆœí™˜ ì‹ ê²½ë§ (RNN, Recurrent Neural Network)\n",
        "\n",
        "ì¸ê³µ ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìˆœí™˜ ì‹ ê²½ë§ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzJSMs2CSkj4"
      },
      "source": [
        "### ì—°ì†í˜• ë°ì´í„° (Sequential Data)\n",
        "\n",
        "- Sequential Dataë€?\n",
        "    - ì–´ë–¤ ìˆœì„œë¡œ ì˜¤ëŠëƒì— ë”°ë¼ì„œ ë‹¨ìœ„ì˜ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ëŠ” ë°ì´í„°\n",
        "    - Non-sequential Data\n",
        "    ![]()\n",
        "    ![]()\n",
        "    - Sequential Data\n",
        "    ![]()\n",
        "    ![]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT9JUEMYULmX"
      },
      "source": [
        "### RNNì˜ êµ¬ì¡°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPrEp2CZIAjT"
      },
      "source": [
        "ê°€ì¥ ê°„ë‹¨í•œ RNNì€ Hidden-layerê°€ 1ê°œì¸ RNNì…ë‹ˆë‹¤.\n",
        "\n",
        "![RNN, Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "\n",
        "í™”ì‚´í‘œ ì™¼ìª½ ê·¸ë¦¼ì—ì„œ $h$ ë¼ê³  ì“´ ì€ë‹‰ì¸µì— ìê¸° ìì‹ ì˜ ì…ë ¥ìœ¼ë¡œ ëŒì•„ê°€ëŠ” $V$ ê°€ ìˆìŠµë‹ˆë‹¤.<br/>\n",
        "ì´ ë£¨í”„ëŠ” ì§€ê¸ˆ ì‹œì ì˜ $t$ì˜ ì¶œë ¥ì„ ìœ„í•´ì„œ $t-1$ ê°’ì´ $U$ì™€ í•©ì³ì ¸ $h$ì— ë‹¤ì‹œ ë°˜ì˜ë˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.<br/>\n",
        "ì¶œë ¥ ë²¡í„°ê°€ ë‹¤ì‹œ ì…ë ¥ë˜ëŠ” íŠ¹ì„± ë•Œë¬¸ì— 'ìˆœí™˜(Recurrent) ì‹ ê²½ë§' ì´ë¼ëŠ” ì´ë¦„ì´ ë¶™ì—ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyIzz4Mt3lM9"
      },
      "source": [
        "### time-step ë³„ë¡œ í¼ì³ì„œ RNN ì•Œì•„ë³´ê¸°\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVipPZJGIEiS"
      },
      "source": [
        "ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ê°€ ì™¼ìª½ ê·¸ë¦¼ì²˜ëŸ¼ í‘œì‹œë˜ì§€ë§Œ ì‹ ê²½ë§ì„ ì‹œì ì— ë”°ë¼ ì´í•´í•˜ë„ë¡ í¼ì³ë³´ë©´ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì²˜ëŸ¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br/>\n",
        "$t-1$ ì‹œì ì—ì„œëŠ” $x_{t-1}$ ì™€ $h_{t-2}$ê°€ ì…ë ¥ë˜ê³  $o_{t-1}$ ì´ ì¶œë ¥ë©ë‹ˆë‹¤.<br/>\n",
        "$t$ ì‹œì ì—ì„œëŠ” $x_t$ ì™€ $h_{t-1}$ ê°€ ì…ë ¥ë˜ê³  $o_t$ ì´ ì¶œë ¥ë©ë‹ˆë‹¤.<br/>\n",
        "$t+1$ ì‹œì ì—ì„œëŠ” $x_{t+1}$ ì™€ $h_t$ ê°€ ì…ë ¥ë˜ê³  $o_{t+1}$ ì´ ì¶œë ¥ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMQ_Fz4jIMJp"
      },
      "source": [
        "t ì‹œì ì˜ RNN ê³„ì¸µì€ ê·¸ ê³„ì¸µìœ¼ë¡œì˜ ì…ë ¥ ë²¡í„° $x_t$ ì™€ 1ê°œ ì „ì˜ RNN ê³„ì¸µì˜ ì¶œë ¥ ë²¡í„° $h_{t-1}$ ë¥¼ ë°›ì•„ë“¤ì…ë‹ˆë‹¤.<br/>\n",
        "ì…ë ¥ëœ ë‘ ë²¡í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ì‹œì ì—ì„œì˜ ì¶œë ¥ì„ ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•©ë‹ˆë‹¤. \n",
        "\n",
        "> $h_t = sigmoid(h_{t-1}W_h + x_tW_x + b)$\n",
        "\n",
        "ê°€ì¤‘ì¹˜ëŠ” $W_h, W_x$ 2ê°œê°€ ìˆìŠµë‹ˆë‹¤. ê°ê° ì…ë ¥ xë¥¼ hë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ $W_x$ì™€ RNNì˜ ì€ë‹‰ì¸µì˜ ì¶œë ¥ì„ ë‹¤ìŒ hë¡œ ë³€í™˜í•´ì£¼ëŠ” $W_h$ ì…ë‹ˆë‹¤. ì‹ ê²½ë§ì—ì„œ biasë„ ìˆë‹¤ëŠ” ê²ƒì€ ìŠì§€ ì•Šìœ¼ì…¨ìœ¼ë¦¬ë¼ ìƒê°í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU7nJUV8IO5A"
      },
      "source": [
        "ì´ ê³¼ì •ì„ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "![rnn2](https://i.imgur.com/nFMF0Nc.png)\n",
        "\n",
        "ì´ë ‡ê²Œ í•˜ë©´ t ì‹œì ì— ìƒì„±ë˜ëŠ” hidden-state ë²¡í„°ì¸ $h_t$ ëŠ” í•´ë‹¹ ì‹œì ê¹Œì§€ ì…ë ¥ëœ ë²¡í„° $x_1, x_2, \\cdots, x_{t-1}, x_t$ ì˜ ì •ë³´ë¥¼ ëª¨ë‘ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.<br/>  Sequential ë°ì´í„°ì˜ ìˆœì„œ ì •ë³´ë¥¼ ëª¨ë‘ ê¸°ì–µí•˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ ë‹¤ë£° ë•Œ RNNì„ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuFkM-j1IWiW"
      },
      "source": [
        "# ë°°ì› ë˜ RNNì„ ê°„ë‹¨í•œ ì½”ë“œë¡œ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \n",
        "import numpy as np\n",
        "\n",
        "class RNN:\n",
        "  def __init__(self, Wx, Wh, b):\n",
        "    self.params = [Wx, Wh, b]\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, x, h_prev):\n",
        "    Wx, Wh, b = self.params\n",
        "    t = np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b\n",
        "    h_next = np.sigmoid(t)\n",
        "\n",
        "    self.cache = (x, h_prev, h_next)\n",
        "    return h_next"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49vchZtrXkMI"
      },
      "source": [
        "### ë‹¤ì–‘í•œ í˜•íƒœì˜ RNN\n",
        "\n",
        "ì‹¤ì œë¡œ ë‹¤ì–‘í•œ í˜•íƒœì˜ RNNì´ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ê°€ì¥ ì™¼ìª½ì— ìœ„ì¹˜í•œ one-to-oneì€ ì‹¤ì§ˆì ìœ¼ë¡œ ìˆœí™˜ì´ ì ìš©ë˜ì§€ëŠ” ì•Šì€ í˜•íƒœì…ë‹ˆë‹¤.<br/>\n",
        "ë‚˜ë¨¸ì§€ 4ê°œì˜ RNNì´ ê°ê° ì–´ë–¤ ë¶„ì•¼ì— ì‚¬ìš©ë˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "![various_rnn](http://karpathy.github.io/assets/rnn/diags.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg522yiFIVYH"
      },
      "source": [
        "1. one-to-many : 1ê°œì˜ ë²¡í„°ë¥¼ ë°›ì•„ Sequentialí•œ ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ì…ë ¥ë°›ì•„ ì´ë¥¼ ì„¤ëª…í•˜ëŠ” ë¬¸ì¥ì„ ë§Œë“¤ì–´ë‚´ëŠ” **ì´ë¯¸ì§€ ìº¡ì…”ë‹(Image captioning)**ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "2. many-to-one : Sequential ë²¡í„°ë¥¼ ë°›ì•„ 1ê°œì˜ ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ë¬¸ì¥ì´ ê¸ì •ì¸ì§€ ë¶€ì •ì¸ì§€ë¥¼ íŒë‹¨í•˜ëŠ” **ê°ì„± ë¶„ì„(Sentiment analysis)**ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "3. many-to-many(1) : Sequential ë²¡í„°ë¥¼ ëª¨ë‘ ì…ë ¥ë°›ì€ ë’¤ Sequential ë²¡í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. **ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤(Sequence-to-Sequence, Seq2Seq) êµ¬ì¡°**ë¼ê³ ë„ ë¶€ë¦…ë‹ˆë‹¤. ë²ˆì—­í•  ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ ë²ˆì—­ëœ ë¬¸ì¥ì„ ë‚´ë†“ëŠ” **ê¸°ê³„ ë²ˆì—­(Machine translation)**ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "4. many-to-many(2) : Sequential ë²¡í„°ë¥¼ ì…ë ¥ë°›ëŠ” ì¦‰ì‹œ Sequential ë²¡í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. **ë¹„ë””ì˜¤ë¥¼ í”„ë ˆì„ë³„ë¡œ ë¶„ë¥˜(Video classification per frame)**í•˜ëŠ” ê³³ì— ì‚¬ìš©ë©ë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSVc5mbjXsr5"
      },
      "source": [
        "### RNNì˜ ì¥ì ê³¼ ë‹¨ì "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFcaHkCQJX1o"
      },
      "source": [
        "- **ì¥ì **\n",
        "\n",
        "RNNì€ ëª¨ë¸ì´ ê°„ë‹¨í•˜ê³  (ì´ë¡ ì ìœ¼ë¡œëŠ”) ì–´ë–¤ ê¸¸ì´ì˜ sequential ë°ì´í„°ë¼ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "í•˜ì§€ë§Œ RNNì€ ëª‡ ê°€ì§€ ë‹¨ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99JK6sFUJdQ"
      },
      "source": [
        "- ë‹¨ì  1 : **ë³‘ë ¬í™”(parallelization) ë¶ˆê°€ëŠ¥**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN5LGndWUYxj"
      },
      "source": [
        "RNN êµ¬ì¡°ê°€ ê°€ì§€ê³  ìˆëŠ” ë‹¨ì  ì¤‘ í•˜ë‚˜ëŠ” ë²¡í„°ê°€ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ëœë‹¤ëŠ” ì ì…ë‹ˆë‹¤.<br/>\n",
        "ì´ëŠ” sequential ë°ì´í„° ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•´ì£¼ëŠ” ìš”ì¸ ì¤‘ í•˜ë‚˜ì´ê¸°ë„ í•©ë‹ˆë‹¤.<br/>\n",
        "í•˜ì§€ë§Œ ì´ëŸ¬í•œ êµ¬ì¡°ëŠ” GPU ì—°ì‚°ì˜ ì¥ì ì¸ ë³‘ë ¬í™”ë¥¼ ë¶ˆê°€ëŠ¥í•˜ê²Œ í•˜ê¸°ë„ í•©ë‹ˆë‹¤.<br/>\n",
        "ê·¸ë ‡ê¸° ë•Œë¬¸ì— RNN ê¸°ë°˜ì˜ ëª¨ë¸ì€ GPU ì—°ì‚°ì„ í•˜ì˜€ì„ ë•Œ ì´ì ì´ ê±°ì˜ ì—†ë‹¤ëŠ” ë‹¨ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl5vjneMJDhO"
      },
      "source": [
        "- ë‹¨ì  2: **ê¸°ìš¸ê¸° í­ë°œ(Exploding Gradient), ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRpwbxXBXuLM"
      },
      "source": [
        "ë‹¨ìˆœ RNNì˜ ì¹˜ëª…ì ì¸ ë¬¸ì œì ì€ ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ë°œìƒí•©ë‹ˆë‹¤.<br/>\n",
        "ì—­ì „íŒŒ ê³¼ì •ì—ì„œ RNNì˜ í™œì„±í™” í•¨ìˆ˜ì¸ $\\tanh$ ì˜ ë¯¸ë¶„ê°’ì„ ì „ë‹¬í•˜ê²Œ ë©ë‹ˆë‹¤. \n",
        "$\\tanh$ ë¥¼ ë¯¸ë¶„í•œ í•¨ìˆ˜ì˜ ê°’ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.<br/>\n",
        "\n",
        "![tanh](https://user-images.githubusercontent.com/45377884/91560164-52a14400-e974-11ea-8bf4-bbfc7fd42deb.png)\n",
        "\n",
        "ìœ„ ê·¸ë˜í”„ì—ì„œ ìµœëŒ“ê°’ì´ 1ì´ê³ , (-4,4) ì´ì™¸ì˜ ë²”ìœ„ì—ì„œëŠ” ê±°ì˜ 0ì— ê°€ê¹Œìš´ ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FShzAI7VV-G"
      },
      "source": [
        "ë¬¸ì œëŠ” ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ì´ ê°’ì„ ë°˜ë³µí•´ì„œ ê³±í•´ì¤€ë‹¤ëŠ” ì ì…ë‹ˆë‹¤.<br/>\n",
        "ì´ Recurrentê°€ 10íšŒ, 100íšŒ ë°˜ë³µëœë‹¤ê³  ë³´ë©´, ì´ ê°’ì˜ 100ì œê³±, 1000ì œê³±ì´ ì‹ ë‚´ë¶€ë¡œ ë“¤ì–´ê°€ê²Œ ë©ë‹ˆë‹¤.<br/>\n",
        "ë§Œì•½ ì´ ê°’ì´ 0.9 ì¼ ë•Œ 10ì œê³±ì´ ëœë‹¤ë©´ 0.349ê°€ ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë˜ë©´ ì‹œí€€ìŠ¤ ì•ìª½ì— ìˆëŠ” hidden-state ë²¡í„°ì—ëŠ” ì—­ì „íŒŒ ì •ë³´ê°€ ê±°ì˜ ì „ë‹¬ë˜ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤.<br/>\n",
        "ì´ëŸ° ë¬¸ì œë¥¼ **ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)**ì´ë¼ê³  í•©ë‹ˆë‹¤.\n",
        "ë°˜ëŒ€ë¡œ ì´ ê°’ì´ 1.1 ì´ë©´ 10ì œê³±ë§Œí•´ë„ 2.59ë°°ë¡œ ì»¤ì§€ê²Œ ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë˜ë©´ ì‹œí€€ìŠ¤ ì•ìª½ì— ìˆëŠ” hidden-state ë²¡í„°ì—ëŠ” ì—­ì „íŒŒ ì •ë³´ê°€ ê³¼í•˜ê²Œ ì „ë‹¬ë©ë‹ˆë‹¤.<br/>\n",
        "ì´ëŸ° ë¬¸ì œë¥¼ **ê¸°ìš¸ê¸° í­ë°œ(Exploding Gradient)**ì´ë¼ê³  í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWGOzd8QVZtA"
      },
      "source": [
        "> \"ê·¸ë ‡ë‹¤ë©´ ì „ë‹¬ë˜ëŠ” ê¸°ìš¸ê¸° ì •ë³´ì˜ í¬ê¸°ë¥¼ ì ì ˆí•˜ê²Œ ì¡°ì ˆí•´ì£¼ë©´ ë˜ì§€ ì•Šì„ê¹Œ?\"\n",
        "\n",
        "ë¼ëŠ” ìƒê°ì´ ë“¤ê¸° ì‹œì‘í•©ë‹ˆë‹¤.<br/>\n",
        "ì „ë‹¬ë˜ëŠ” ì—­ì „íŒŒ ê°’ì˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ëŠ” Gateë¥¼ ë§Œë“¤ì–´ì„œ RNNì˜ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ìëŠ” ë°©í–¥ì—ì„œ ì¶œë°œí•œ ê²ƒì´ ë°”ë¡œ **ì¥ë‹¨ê¸° ê¸°ì–µë§(Long-Short Term Memory, LSTM)**ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5VoE0ohXw2d"
      },
      "source": [
        "## 3. LSTM & GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svBQyfcmYMLJ"
      },
      "source": [
        "### LSTM (Long Term Short Memory, ì¥ë‹¨ê¸°ê¸°ì–µë§)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PqJ2l6YSLx"
      },
      "source": [
        "ì´ë ‡ê²Œ **RNNì— Gateë¥¼ ì¶”ê°€í•œ ëª¨ë¸ì„ LSTM**ì´ë¼ê³  í•©ë‹ˆë‹¤.<br/>\n",
        "ìš”ì¦˜ì—ëŠ” ë‹¨ìˆœí•œ RNNì€ ì‚¬ìš©í•˜ì§€ ì•Šê³  ëŒ€ë¶€ë¶„ ì¥ê¸° ë‹¨ê¸° ê¸°ì–µ ì¥ì¹˜ (LSTM)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.<br/>\n",
        "ìš”ì¦˜ RNNì´ë¼ê³  í•˜ë©´ ë‹¹ì—°íˆ LSTMì´ë‚˜ ì´í›„ì— ë°°ìš¸ GRUë¥¼ ì§€ì¹­í•  ì •ë„ë¡œ LSTMì´ ëŒ€í‘œì ì¸ RNNì˜ ëª¨ë¸ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.<br/>\n",
        "ì˜¤íˆë ¤ ì „ì— ë°°ìš´ RNNì„ `ê¸°ë³¸ì ì¸ RNN(Vanilla RNN)`ì´ë¼ê³  ë”°ë¡œ êµ¬ë³„í•˜ì—¬ í‘œí˜„í•˜ê¸°ë„ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aocYxj7AVtrf"
      },
      "source": [
        "- **LSTMì˜ êµ¬ì¡°**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2GN3aS0V1dT"
      },
      "source": [
        "ë¨¼ì € ê·¸ë¦¼ì„ í†µí•´ LSTM ì…€ í•˜ë‚˜ì˜ êµ¬ì¡°ë¥¼ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile5.uf.tistory.com%2Fimage%2F9905CF385BD5F5EC027F20\"/>\n",
        "\n",
        "RNNì˜ ì…€ êµ¬ì¡°ë³´ë‹¤ ë­”ê°€ ìƒë‹¹íˆ ë³µì¡í•´ì¡ŒìŠµë‹ˆë‹¤.\n",
        "\n",
        "ìœ„ì—ì„œ ì‚´í´ë³¸ ê²ƒì²˜ëŸ¼ LSTMì´ ë“±ì¥í•œ ë°°ê²½ì€ ì•ìª½ ì‹œí€€ìŠ¤ê¹Œì§€ ì—­ì „íŒŒ ì •ë³´ê°€ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì§€ ì•ŠëŠ” [ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing gradient)](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNZi7DwbXRI2"
      },
      "source": [
        "LSTMì€ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 3ê°€ì§€ ê²Œì´íŠ¸(gate)ë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. ê° ê²Œì´íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
        "\n",
        "1. forget gate ($f_t$): ê³¼ê±° ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ìœ ì§€í•  ê²ƒì¸ê°€?\n",
        "2. input gate ($i_t$) : ìƒˆë¡œ ì…ë ¥ëœ ì •ë³´ëŠ” ì–¼ë§ˆë§Œí¼ í™œìš©í•  ê²ƒì¸ê°€?\n",
        "3. output gate ($o_t$) : ë‘ ì •ë³´ë¥¼ ê³„ì‚°í•˜ì—¬ ë‚˜ì˜¨ ì¶œë ¥ ì •ë³´ë¥¼ ì–¼ë§ˆë§Œí¼ ë„˜ê²¨ì¤„ ê²ƒì¸ê°€?\n",
        "\n",
        "ê·¸ë¦¬ê³  hidden-state ë§ê³ ë„ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì§ì ‘ ê±°ì¹˜ì§€ ì•ŠëŠ” ìƒíƒœì¸ cell-state ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.<br/>\n",
        "cell-stateëŠ” ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ì •ë³´ ì†ì‹¤ì´ ì—†ê¸° ë•Œë¬¸ì— **ìµœê·¼(short) ì´ë²¤íŠ¸ì— ë¹„ì¤‘ì„ ê²°ì •í•  ìˆ˜ ìˆìœ¼ë©´ì„œ ë™ì‹œì— ì˜¤ë˜ëœ(long) ì •ë³´ë¥¼ ì™„ì „íˆ ìƒì§€ ì•Šì„ ìˆ˜ ìˆë‹¤**ëŠ” LSTMì˜ ì¥ì ì„ ì‚´ë¦´ ìˆ˜ ìˆëŠ” í•„ìˆ˜ì  ìš”ì†Œê°€ ë˜ì—ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1vIvtc0YYjJ"
      },
      "source": [
        "- **LSTMì˜ ì—­ì „íŒŒ**\n",
        "\n",
        "*(ë‹¤ì†Œ ë³µì¡í•˜ë¯€ë¡œ ë‹¹ì¥ì€ LSTMì˜ ì—­ì „íŒŒ ê³¼ì •ì„ ì „ë¶€ ë‹¤ ì´í•´í•˜ë ¤ í•˜ì§€ ì•Šì•„ë„ ê´œì°®ìŠµë‹ˆë‹¤!)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsJpuM01Y404"
      },
      "source": [
        "<img src=\"http://i.imgur.com/2BZtc2l.gif\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-Jo5pjsYxWj"
      },
      "source": [
        "- **LSTMì˜ ì‚¬ìš©**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcSKg-1kYxOt"
      },
      "source": [
        "LSTMì€ ì‹¤ì œë¡œ êµ‰ì¥íˆ ë§ì€ ê³³ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "ì—¬ëŸ¬ ì–¸ì–´ ëª¨ë¸ì—ì„œ LSTMì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.<br/>\n",
        "Gateê°€ ì ìš©ë˜ì§€ ì•Šì€ RNN, ì¦‰ Vanilla RNNì€ 10~20 ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì¥ì— ëŒ€í•œ ë¶„ë¥˜/ìƒì„±/ë²ˆì—­ ë“±ì˜ ì„±ëŠ¥ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤.<br/>\n",
        "Vanilla RNNì´ ê°€ì§€ê³  ìˆëŠ” ê¸°ìš¸ê¸° ì†Œì‹¤/í­ë°œ ë¬¸ì œ ë•Œë¬¸ì…ë‹ˆë‹¤.<br/>\n",
        "\n",
        "ì–¸ì–´ ëª¨ë¸ ë¿ë§Œ ì•„ë‹ˆë¼ ì‹ ê²½ë§ì„ í™œìš©í•œ ì‹œê³„ì—´ ì•Œê³ ë¦¬ì¦˜ì—ëŠ” ëŒ€ë¶€ë¶„ LSTMì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHoCbrgyYdeE"
      },
      "source": [
        "### GRU (Gated Recurrent Unit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI_lVKKRYxPv"
      },
      "source": [
        "í•œí¸, ì´ LSTMì˜ ê°„ì†Œí•œ ë²„ì „ì¸ GRUë„ ê°€ë³ê²Œ ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤.<br/>\n",
        "ê·¸ë¦¼ì„ í†µí•´ GRU ì…€ì˜ êµ¬ì¡°ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvD-d3PhYlKm"
      },
      "source": [
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile7.uf.tistory.com%2Fimage%2F99F0EC3E5BD5F6460255CF\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T74E2DdeY57x"
      },
      "source": [
        "- GRU ì…€ì˜ íŠ¹ì§•\n",
        "\n",
        "1. LSTMì—ì„œ ìˆì—ˆë˜ cell-stateê°€ ì‚¬ë¼ì¡ŒìŠµë‹ˆë‹¤. cell-state ë²¡í„° $c_t$ â€‹ì™€ hidden-state ë²¡í„° $h_t$â€‹ê°€ í•˜ë‚˜ì˜ ë²¡í„° $h_t$â€‹ë¡œ í†µì¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "2. í•˜ë‚˜ì˜ Gate $z_t$ê°€ forget, input gateë¥¼ ëª¨ë‘ ì œì–´í•©ë‹ˆë‹¤.  $z_t$ê°€ 1ì´ë©´ forget ê²Œì´íŠ¸ê°€ ì—´ë¦¬ê³ , input ê²Œì´íŠ¸ê°€ ë‹«íˆê²Œ ë˜ëŠ” ê²ƒê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ $z_t$ê°€ 0ì´ë©´ input ê²Œì´íŠ¸ë§Œ ì—´ë¦¬ëŠ” ê²ƒê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
        "3. GRU ì…€ì—ì„œëŠ” output ê²Œì´íŠ¸ê°€ ì—†ì–´ì¡ŒìŠµë‹ˆë‹¤. ëŒ€ì‹ ì— ì „ì²´ ìƒíƒœ ë²¡í„° $h_t$ê°€ ê° time-stepì—ì„œ ì¶œë ¥ë˜ë©°, ì´ì „ ìƒíƒœì˜ $h_{t-1}$ì˜ ì–´ëŠ ë¶€ë¶„ì´ ì¶œë ¥ë  ì§€ ìƒˆë¡­ê²Œ ì œì–´í•˜ëŠ” Gateì¸ $r_t$ ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9SIBLuVYmjS"
      },
      "source": [
        "### LSTM ì½”ë“œ ì‹¤ìŠµ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjz6_OkgbHU9"
      },
      "source": [
        "ì´ì œë¶€í„° TensorFlowì™€ Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ë¡œ RNNì„ í›ˆë ¨ì‹œì¼œ ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "- https://www.tensorflow.org/guide/keras/rnn\n",
        "- https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
        "- https://keras.io/api/layers/recurrent_layers/lstm/\n",
        "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL4Mv82gxlrv"
      },
      "source": [
        "ì‹œí€€ìŠ¤ëŠ” ì£¼ê°€ë¶€í„° í…ìŠ¤íŠ¸ê¹Œì§€ ë‹¤ì–‘í•œ ëª¨ì–‘ê³¼ í˜•íƒœë¡œ ì œê³µë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì£¼ë¡œ í…ìŠ¤íŠ¸ì— ì´ˆì ì„ ë§ì¶° ê³µë¶€í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì™œëƒí•˜ë©´ í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì€ ì‹ ê²½ë§ì˜ ê°•ì ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë¨¼ì € TensorFlow íŠœí† ë¦¬ì–¼ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ë¶„ë¥˜ ì‘ì—…ë¶€í„° ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D18BNmDLxowg"
      },
      "source": [
        "- **Kerasë¥¼ ì´ìš©í•œ RNN/LSTM ê°ì • ë¶„ë¥˜(Sentiment classification)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtSEKGkIx17L"
      },
      "source": [
        "'''\n",
        "# IMDB ê°ì„± ë¶„ë¥˜ ì‘ì—…ì— ëŒ€í•œ LSTM ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "ë°ì´í„° ì§‘í•©ì´ ì‚¬ì‹¤ ë„ˆë¬´ ì‘ì•„ì„œ LSTMì´ ê°•ì ì„ ë°œíœ˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
        "TF-IDF + LogRegì™€ ê°™ì€ ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ë°©ë²•ì´ LSTMì— ë¹„í•´ í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤.\n",
        "**Notes**\n",
        "- RNNì€ ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸° ì„ íƒì´ ì¤‘ìš”í•˜ê³ , ì†ì‹¤ ë° ìµœì í™” ë„êµ¬ ì„ íƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì¼ë¶€ êµ¬ì„±ì€ ìˆ˜ë ´ë˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.\n",
        "- êµìœ¡ ì¤‘ LSTM ì†ì‹¤ ê°ì†Œ íŒ¨í„´ì€ CNN/MLP/etcì—ì„œ ë³´ëŠ” ê²ƒëŠ” ìƒë‹¹íˆ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imd\n",
        "\n",
        "# ì´ ë‹¨ì–´ ë­í¬ ìˆ˜ ë’¤ì— í…ìŠ¤íŠ¸ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì˜ë¼ëƒ…ë‹ˆë‹¤(ë‹¨ì–´ ë“±ì¥ ìˆœìœ„ : max_feature)\n",
        "# ì°¸ì¡°ë§í¬ : https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data\n",
        "max_features = 20000\n",
        "# ìµœëŒ€ ë‹¨ì–´ ê¸¸ì´\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT7elx2Tx6hm"
      },
      "source": [
        "print('Pad Sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape: ', x_train.shape)\n",
        "print('x_test shape: ', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32_rru-5x-5L"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpqT7b_KyBJh"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(max_features, 128))\n",
        "# model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Embedding(max_features, 128),\n",
        "  tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq459yjQyDyj"
      },
      "source": [
        "unicorns = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size, \n",
        "          epochs=3, \n",
        "          validation_data=(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDXWi9TByHBj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(unicorns.history['loss'])\n",
        "plt.plot(unicorns.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQdFbh68xyHc"
      },
      "source": [
        "- **Kerasë¥¼ ì´ìš©í•œ LSTM í…ìŠ¤íŠ¸ ìƒì„±ê¸°**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnYYB89fYmLe"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(unicorns.history['loss'])\n",
        "plt.plot(unicorns.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySeBtkvIyW-R"
      },
      "source": [
        "# max lengthë¥¼ ì´ìš©í•˜ì—¬ ë¬¸ìì—´ì˜ í¬ê¸° ì •ë ¬\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7wqty4RyZoI"
      },
      "source": [
        "# LSTM ëª¨ë¸ ì œì‘\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP0huQTyya9e"
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQr_UmPaycjd"
      },
      "source": [
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=60,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvuSPMf_ZIji"
      },
      "source": [
        "## 4. RNN êµ¬ì¡°ì— Attention ì ìš©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7LxiXv-kK9j"
      },
      "source": [
        "### ê¸°ì¡´ RNN ê¸°ë°˜(LSTM, GRU) ë²ˆì—­ ëª¨ë¸ì˜ ë‹¨ì "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc8W05Lab6Iw"
      },
      "source": [
        "RNNì´ ê°€ì§„ ê°€ì¥ í° ë‹¨ì  ì¤‘ í•˜ë‚˜ëŠ” ê¸°ìš¸ê¸° ì†Œì‹¤ë¡œë¶€í„° ë‚˜íƒ€ë‚˜ëŠ” **ì¥ê¸° ì˜ì¡´ì„±(Long-term dependency)** ë¬¸ì œì…ë‹ˆë‹¤.<br/>\n",
        "ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë€ ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆ ê²½ìš° ì• ë‹¨ì–´ì˜ ì •ë³´ë¥¼ ìƒì–´ë²„ë¦¬ê²Œ ë˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤.<br/>\n",
        "ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ê²ƒì´ ì…€ êµ¬ì¡°ë¥¼ ê°œì„ í•œ LSTMê³¼ GRUì…ë‹ˆë‹¤.<br/>\n",
        "ê¸°ê³„ ë²ˆì—­ì—ì„œ RNN ê¸°ë°˜ì˜ ëª¨ë¸(LSTM, GRU)ì´ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3ox-uhBb72u"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/45377884/86040995-f27b4800-ba7f-11ea-8ca1-67b2517573eb.gif\" alt=\"seq2seq_6\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyaxMWVVkYwm"
      },
      "source": [
        "### Attentionì˜ ë“±ì¥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEoreUZQb-bz"
      },
      "source": [
        "ìœ„ êµ¬ì¡°ì˜ ë¬¸ì œëŠ” ê³ ì • ê¸¸ì´ì˜ hidden-state ë²¡í„°ì— ëª¨ë“  ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë‹´ì•„ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.<br/>\n",
        "ì•„ë¬´ë¦¬ LSTM, GRUê°€ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ ê°œì„ í•˜ì˜€ë”ë¼ë„ ë¬¸ì¥ì´ ë§¤ìš° ê¸¸ì–´ì§€ë©´(30-50 ë‹¨ì–´) ëª¨ë“  ë‹¨ì–´ ì •ë³´ë¥¼ ê³ ì • ê¸¸ì´ì˜ hidden-state ë²¡í„°ì— ë‹´ê¸° ì–´ë µìŠµë‹ˆë‹¤.<br/>\n",
        "ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ê³ ì•ˆëœ ë°©ë²•ì´ ë°”ë¡œ **Attention(ì–´í…ì…˜)** ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Au-MKZRb_Db"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/45377884/86040873-b942d800-ba7f-11ea-9f59-ee23923f777e.gif\" alt=\"seq2seq_7\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2F-34z6cB2Y"
      },
      "source": [
        "Attentionì€ ê° ì¸ì½”ë”ì˜ Time-step ë§ˆë‹¤ ìƒì„±ë˜ëŠ” hidden-state ë²¡í„°ë¥¼ ê°„ì§í•©ë‹ˆë‹¤.<br/>\n",
        "ì…ë ¥ ë‹¨ì–´ê°€ Nê°œë¼ë©´ Nê°œì˜ hidden-state ë²¡í„°ë¥¼ ëª¨ë‘ ê°„ì§í•˜ê²Œ ë©ë‹ˆë‹¤.<br/>\n",
        "ëª¨ë“  ë‹¨ì–´ê°€ ì…ë ¥ë˜ë©´ ìƒì„±ëœ hidden-state ë²¡í„°ë¥¼ ëª¨ë‘ ë””ì½”ë”ì— ë„˜ê²¨ì¤ë‹ˆë‹¤.\n",
        "\n",
        "ë””ì½”ë”ëŠ” ë°›ì€ Nê°œì˜ hidden-state ë²¡í„°ë¥¼ ì–´ë–»ê²Œ í™œìš©í• ê¹Œìš”?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJOv0ZMGcESi"
      },
      "source": [
        "- **ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì•„ì´ë””ì–´ ë‘˜ëŸ¬ë³´ê¸°**\n",
        "\n",
        "ì ì‹œ ëŒì•„ê°€ ê²€ìƒ‰ ì‹œìŠ¤í…œì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤.<br/>\n",
        "ì•„ë˜ëŠ” êµ¬ê¸€ì—ì„œ _\"what is attention in nlp\"_ ë¼ëŠ” ê²€ìƒ‰ì–´ë¥¼ êµ¬ê¸€ì— ì…ë ¥í–ˆì„ ë•Œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ ì´ë¯¸ì§€ì…ë‹ˆë‹¤.\n",
        "\n",
        "<img src=\"https://i.imgur.com/JdCQr1l.png\" alt=\"retrieval_system\" width=\"600\" />\n",
        "\n",
        "ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ê²€ìƒ‰ ì‹œìŠ¤í…œì€ ì•„ë˜ì™€ ê°™ì€ 3ë‹¨ê³„ë¥¼ ê±°ì³ ì‘ë™í•©ë‹ˆë‹¤.\n",
        "\n",
        "1. ì°¾ê³ ì í•˜ëŠ” ì •ë³´ì— ëŒ€í•œ ê²€ìƒ‰ì–´(Query)ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.\n",
        "2. ê²€ìƒ‰ ì—”ì§„ì€ ê²€ìƒ‰ì–´ì™€ ê°€ì¥ ë¹„ìŠ·í•œ í‚¤ì›Œë“œ(Key)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
        "3. ê·¸ë¦¬ê³  í•´ë‹¹ í‚¤ì›Œë“œ(Key)ì™€ ì—°ê²°ëœ í˜ì´ì§€(Value)ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NabC-Q8-hiu7",
        "outputId": "c84d90ee-5694-407a-e1d2-4f76b76ecc6e"
      },
      "source": [
        "# ë” ì•Œì•„ë³´ê¸° : íŒŒì´ì¬ì˜ ë”•ì…”ë„ˆë¦¬ë„ ë¹„ìŠ·í•œ í˜•íƒœë¡œ ì‘ë™í•©ë‹ˆë‹¤.\n",
        "# Query('a')ë¥¼ ë˜ì§€ë©´ ë”•ì…”ë„ˆë¦¬ì—ì„œ ë™ì¼í•œ Key('a')ë¥¼ ì°¾ì€ ë’¤ Value(123)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "dict1 = {'a':123, 'b':425, 'c':236, 'd':945}\n",
        "dict1['a']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_4UTfufkH4b"
      },
      "source": [
        "### ë””ì½”ë”ì—ì„œ Attentionì´ ë™ì‘í•˜ëŠ” ë°©ë²•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8cU0WZ8ez7m"
      },
      "source": [
        "ë””ì½”ë”ì—ì„œ ë‹¨ì–´ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ë””ì½”ë”ì˜ ê° time-step ë§ˆë‹¤ì˜ hidden-state ë²¡í„°ëŠ” ì¿¼ë¦¬(query)ë¡œ ì‘ìš©í•©ë‹ˆë‹¤.<br/>\n",
        "ì¸ì½”ë”ì—ì„œ ë„˜ì–´ì˜¨ Nê°œì˜ hidden-state ë²¡í„°ë¥¼ í‚¤(key)ë¡œ ì—¬ê¸°ê³  ì´ë“¤ê³¼ì˜ ì—°ê´€ì„±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.<br/>\n",
        "ì´ ë•Œ ê³„ì‚°ì€ ë‚´ì (dot-product)ì„ ì‚¬ìš©í•˜ê³  ë‚´ì ì˜ ê²°ê³¼ë¥¼ Attention ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.<br/>\n",
        "\n",
        "ì•„ë˜ëŠ” ë””ì½”ë” ì²« ë‹¨ì–´ \"I\"(`Time-step 4`)ì— ëŒ€í•œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ê°€ êµ¬í•´ì§€ëŠ” ê³¼ì •ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_z9iXJ1e2G3"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/45377884/86044868-ae8b4180-ba85-11ea-9fee-2977edfd47ce.gif\" alt=\"seq2seq_img\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEEOdnoZnTY7"
      },
      "source": [
        "1. ì¿¼ë¦¬(Query)ì¸ ë””ì½”ë”ì˜ hidden-state ë²¡í„°, í‚¤(Key)ì¸ ì¸ì½”ë”ì—ì„œ ë„˜ì–´ì˜¨ hidden-state ë²¡í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
        "2. ê°ê°ì˜ ë²¡í„°ë¥¼ ë‚´ì í•œ ê°’ì„ êµ¬í•©ë‹ˆë‹¤.\n",
        "3. ì´ ê°’ì— ì†Œí”„íŠ¸ë§¥ìŠ¤(softmax) í•¨ìˆ˜ë¥¼ ì·¨í•´ì¤ë‹ˆë‹¤.\n",
        "4. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì·¨í•˜ì—¬ ë‚˜ì˜¨ ê°’ì— ë°¸ë¥˜(Value)ì— í•´ë‹¹í•˜ëŠ” ì¸ì½”ë”ì—ì„œ ë„˜ì–´ì˜¨ hidden-state ë²¡í„°ë¥¼ ê³±í•´ì¤ë‹ˆë‹¤. \n",
        "5. ì´ ë²¡í„°ë¥¼ ëª¨ë‘ ë”í•´ì¤ë‹ˆë‹¤. ì´ ë²¡í„°ì˜ ì„±ë¶„ ì¤‘ì—ëŠ” ì¿¼ë¦¬-í‚¤ ì—°ê´€ì„±ì´ ë†’ì€ ë°¸ë¥˜ ë²¡í„°ì˜ ì„±ë¶„ì´ ë” ë§ì´ ë“¤ì–´ìˆê²Œ ë©ë‹ˆë‹¤.\n",
        "6. (ê·¸ë¦¼ì—ëŠ” ë‚˜ì™€ìˆì§€ ì•Šì§€ë§Œ) ìµœì¢…ì ìœ¼ë¡œ 5ì—ì„œ ìƒì„±ëœ ë²¡í„°ì™€ ë””ì½”ë”ì˜ hidden-state ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥ ë‹¨ì–´ë¥¼ ê²°ì •í•˜ê²Œ ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkDHPM3Ie5Kz"
      },
      "source": [
        "ë””ì½”ë”ëŠ” ì¸ì½”ë”ì—ì„œ ë„˜ì–´ì˜¨ ëª¨ë“  Hidden state ë²¡í„°ì— ëŒ€í•´ ìœ„ì™€ ê°™ì€ ê³„ì‚°ì„ ì‹¤ì‹œí•©ë‹ˆë‹¤.<br/>\n",
        "ê·¸ë ‡ê¸° ë•Œë¬¸ì— Time-stepë§ˆë‹¤ ì¶œë ¥í•  ë‹¨ì–´ê°€ ì–´ë–¤ ì¸ì½”ë”ì˜ ì–´ë–¤ ë‹¨ì–´ ì •ë³´ì™€ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€, ì¦‰ ì–´ë–¤ ë‹¨ì–´ì— **ì§‘ì¤‘(Attention)**í•  ì§€ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br/>\n",
        "Attentionì„ í™œìš©í•˜ë©´ ë””ì½”ë”ê°€ ì¸ì½”ë”ì— ì…ë ¥ë˜ëŠ” ëª¨ë“  ë‹¨ì–´ì˜ ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì•„ë˜ëŠ” ì˜ˆì‹œë¡œ ì œì‹œë˜ì—ˆë˜ ë¬¸ì¥ì„ ë²ˆì—­(**`Je suis etudiant => I am a student`**)í–ˆì„ ë•Œ ê° ë‹¨ì–´ë§ˆë‹¤ì˜ Attention ìŠ¤ì½”ì–´ë¥¼ ì‹œê°í™” í•œ ê·¸ë¦¼ì…ë‹ˆë‹¤.<br/>\n",
        "\n",
        "> _\"I\"_ -> _\"Je\"_ <br/>\n",
        "> _\"am\"_ -> _\"suis\"_<br/>\n",
        "> _\"a\"_ -> _\"suis\", \"etudiant\"_<br/>\n",
        "> _\"student\"_ -> _\"etudiant\"_\n",
        "\n",
        "ì™¼ìª½ ë‹¨ì–´ê°€ ìƒì„±ë  ë•Œ ì˜¤ë¥¸ìª½ ë‹¨ì–´ì™€ ì—°ê´€ë˜ì–´ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXI1tj2we7hR"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/45377884/86047018-29a22700-ba89-11ea-98ee-a90b2fb70a23.gif\" alt=\"attn_visualization\" width=\"500\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOAShST2e8Ik"
      },
      "source": [
        "### RNN(LSTM) with Attention ì½”ë“œ ì‹¤ìŠµ\n",
        "\n",
        "ìŠ¤í˜ì¸ì–´-ì˜ì–´ ë²ˆì—­í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYnEXYKGewLQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq5-fp4gfKxz"
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqxC2SgsfPCp"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                 if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYpqyskDfQqi"
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"Â¿Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iSyCfNrfSvT"
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')]\n",
        "                for line in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66wKsoJYfT5p"
      },
      "source": [
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEBx2J0OfUUu"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOjGqWpsfVmK"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFJWIindfXJ5"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6SFHicNfYa1"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWEEaYHlfalW"
      },
      "source": [
        "- êµ¬ì¡°ì™€ ê´€ë ¨ëœ íŒŒë¼ë¯¸í„° ì„¤ì •í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc3UtRN7fiI-"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpvSXgBBfi6o"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZBV4UUBfgkr"
      },
      "source": [
        "- ì¸ì½”ë” êµ¬í˜„í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE3-rfGWfm1k"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehWNiQ_ffokz"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
        "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKG3Y_BafqBJ"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZuX1KCdfr8h"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units)\", attention_result.shape)\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1)\", attention_weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjbGgc5Efuor"
      },
      "source": [
        "- **ë””ì½”ë” êµ¬í˜„**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l1Zd0Nkft6G"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GK7-WsOfx4k"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABDicArafzt_"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTeGGZS1f2M6"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20YBdgsYf5gr"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "      \n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0EujLQZf8NJ"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIGrhbkrf-_u"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnamxQ1sgAkK"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ6MY4FtgDe7"
      },
      "source": [
        "translate(u'hace mucho frio aqui.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V6aKQ4BgFq3"
      },
      "source": [
        "translate(u'esta es mi vida.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyVjn9UeaH0h"
      },
      "source": [
        "## Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zAIH2w0YzRe"
      },
      "source": [
        "- ì–¸ì–´ ëª¨ë¸ (Language Model)\n",
        "    - í†µê³„ ê¸°ë°˜ì˜ ì–¸ì–´ ëª¨ë¸(Statistical Language Model)\n",
        "    - ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸(Neural Network Language Model) \n",
        "\n",
        "- ìˆœí™˜ ì‹ ê²½ë§ (Recurrent Neural Network, RNN)\n",
        "    - RNNì˜ êµ¬ì¡°\n",
        "    - RNNì˜ ì¥ì ê³¼ ë‹¨ì \n",
        "        - ê¸°ìš¸ê¸° ì†Œì‹¤(Gradient Vanishing)\n",
        "\n",
        "- LSTM & GRU\n",
        "    - LSTM\n",
        "        - Cell state\n",
        "    - GRU\n",
        "\n",
        "- Attention\n",
        "    - Attention\n",
        "        - ì¥ê¸° ì˜ì¡´ì„±(Long-term Dependency)\n",
        "    - Query, Key, Vector\n",
        "        - ì–´ë–¤ ë²¡í„°ê°€ ê° ìš”ì†Œì— í•´ë‹¹ë ê¹Œìš”?"
      ]
    }
  ]
}