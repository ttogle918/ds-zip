{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "boston_house_ML_DL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPAdE/6RqDgmFK9LCpe85RT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttogle918/ds-section4-sprint1/blob/master/n411/boston_house_ML_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boston 주택 가격 예측\n",
        "\n",
        "속성 정보 : http://lib.stat.cmu.edu/datasets/boston\n"
      ],
      "metadata": {
        "id": "3CYbdBrYeDnA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sQaFk_4RdS-Y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "boston_housing = tf.keras.datasets.boston_housing\n",
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPhOwjljjn-K",
        "outputId": "8c9da1dc-bdd0-4bad-efc4-6e7094fec5af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1.23247,   0.     ,   8.14   ,   0.     ,   0.538  ,   6.142  ,\n",
              "        91.7    ,   3.9769 ,   4.     , 307.     ,  21.     , 396.9    ,\n",
              "        18.72   ])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train), len(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeHoqTCMdnko",
        "outputId": "5dd51842-6828-42be-a438-63070112b975"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML\n",
        "section2 - sprint1\n"
      ],
      "metadata": {
        "id": "KNHz9JoMds5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "ridge = RidgeCV(alphas=[0.01], normalize=True, cv=5)\n",
        "\n",
        "ridge.fit(x_train, y_train)\n",
        "\n",
        "y_pred = ridge.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4KnB1vnePDB",
        "outputId": "ce307fd4-7d27-49ea-a94e-690b040f6ba4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"alpha: \", ridge.alpha_)\n",
        "print(\"best score: \", ridge.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ov2F7oA2kTJi",
        "outputId": "96705287-0c3a-42ca-feef-49496dfeec9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpha:  0.01\n",
            "best score:  0.6990651459393651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델로 테스트 에러(MAE) 계산\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f'테스트 에러: {mae:.2f}')\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE : \", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "449Vuln0eYba",
        "outputId": "4f0ab2fe-391e-4e73-de08-ddb4d400bbfc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 에러: 3.44\n",
            "MSE :  22.86814930777767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 딥러닝"
      ],
      "metadata": {
        "id": "wjOCTo4ofbk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "7c6oVjBbfdtI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1)) # regression"
      ],
      "metadata": {
        "id": "ceCdHvJNgG3k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
      ],
      "metadata": {
        "id": "sp6EnjvUguTa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train_scaled, y_train, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgeW_T6uhADY",
        "outputId": "58331ae0-8944-4597-d7f3-4451cdeb8c7b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 6.1927 - mae: 1.7420\n",
            "Epoch 2/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 6.0436 - mae: 1.7432\n",
            "Epoch 3/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.9653 - mae: 1.6963\n",
            "Epoch 4/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.9311 - mae: 1.7049\n",
            "Epoch 5/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.9376 - mae: 1.7156\n",
            "Epoch 6/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.9560 - mae: 1.7189\n",
            "Epoch 7/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.8814 - mae: 1.6931\n",
            "Epoch 8/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.8331 - mae: 1.6965\n",
            "Epoch 9/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.8177 - mae: 1.6934\n",
            "Epoch 10/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.8454 - mae: 1.6941\n",
            "Epoch 11/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.7788 - mae: 1.6990\n",
            "Epoch 12/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.7986 - mae: 1.6926\n",
            "Epoch 13/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.7876 - mae: 1.6776\n",
            "Epoch 14/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.7331 - mae: 1.6904\n",
            "Epoch 15/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.6678 - mae: 1.6684\n",
            "Epoch 16/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.7763 - mae: 1.6858\n",
            "Epoch 17/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.7206 - mae: 1.6866\n",
            "Epoch 18/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.6422 - mae: 1.6588\n",
            "Epoch 19/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.6346 - mae: 1.6667\n",
            "Epoch 20/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.7152 - mae: 1.6863\n",
            "Epoch 21/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.6149 - mae: 1.6524\n",
            "Epoch 22/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.5941 - mae: 1.6750\n",
            "Epoch 23/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.6278 - mae: 1.6612\n",
            "Epoch 24/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.5579 - mae: 1.6602\n",
            "Epoch 25/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.6232 - mae: 1.6867\n",
            "Epoch 26/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.5320 - mae: 1.6578\n",
            "Epoch 27/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.4965 - mae: 1.6558\n",
            "Epoch 28/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.5443 - mae: 1.6591\n",
            "Epoch 29/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.5329 - mae: 1.6505\n",
            "Epoch 30/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.5294 - mae: 1.6632\n",
            "Epoch 31/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.4406 - mae: 1.6562\n",
            "Epoch 32/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.4170 - mae: 1.6366\n",
            "Epoch 33/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.3422 - mae: 1.6213\n",
            "Epoch 34/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.5533 - mae: 1.6651\n",
            "Epoch 35/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.4353 - mae: 1.6480\n",
            "Epoch 36/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.3285 - mae: 1.6235\n",
            "Epoch 37/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.3209 - mae: 1.6112\n",
            "Epoch 38/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.3116 - mae: 1.6239\n",
            "Epoch 39/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.2941 - mae: 1.6160\n",
            "Epoch 40/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.2184 - mae: 1.5953\n",
            "Epoch 41/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.2192 - mae: 1.6040\n",
            "Epoch 42/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.2949 - mae: 1.6347\n",
            "Epoch 43/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.2421 - mae: 1.6032\n",
            "Epoch 44/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.3101 - mae: 1.6177\n",
            "Epoch 45/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.2645 - mae: 1.6240\n",
            "Epoch 46/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.1787 - mae: 1.5994\n",
            "Epoch 47/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.0900 - mae: 1.5910\n",
            "Epoch 48/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.2528 - mae: 1.6101\n",
            "Epoch 49/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.1086 - mae: 1.5970\n",
            "Epoch 50/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.0610 - mae: 1.5766\n",
            "Epoch 51/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.0762 - mae: 1.5794\n",
            "Epoch 52/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.9877 - mae: 1.5616\n",
            "Epoch 53/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.0151 - mae: 1.5762\n",
            "Epoch 54/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.0630 - mae: 1.5548\n",
            "Epoch 55/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 5.0663 - mae: 1.6009\n",
            "Epoch 56/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.9481 - mae: 1.5690\n",
            "Epoch 57/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.9187 - mae: 1.5512\n",
            "Epoch 58/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.9309 - mae: 1.5508\n",
            "Epoch 59/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.9915 - mae: 1.5816\n",
            "Epoch 60/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.9300 - mae: 1.5626\n",
            "Epoch 61/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.9311 - mae: 1.5481\n",
            "Epoch 62/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.8174 - mae: 1.5504\n",
            "Epoch 63/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.8165 - mae: 1.5351\n",
            "Epoch 64/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.7993 - mae: 1.5551\n",
            "Epoch 65/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.8480 - mae: 1.5361\n",
            "Epoch 66/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.8659 - mae: 1.5683\n",
            "Epoch 67/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.7122 - mae: 1.5244\n",
            "Epoch 68/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.6413 - mae: 1.5152\n",
            "Epoch 69/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.6978 - mae: 1.5149\n",
            "Epoch 70/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.6822 - mae: 1.5359\n",
            "Epoch 71/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.5948 - mae: 1.5133\n",
            "Epoch 72/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.5929 - mae: 1.5123\n",
            "Epoch 73/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.5213 - mae: 1.5070\n",
            "Epoch 74/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.5373 - mae: 1.5062\n",
            "Epoch 75/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.5716 - mae: 1.5058\n",
            "Epoch 76/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.4737 - mae: 1.4959\n",
            "Epoch 77/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.4802 - mae: 1.4936\n",
            "Epoch 78/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.4692 - mae: 1.4932\n",
            "Epoch 79/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.5227 - mae: 1.4872\n",
            "Epoch 80/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.5623 - mae: 1.5239\n",
            "Epoch 81/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.6799 - mae: 1.5306\n",
            "Epoch 82/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.5943 - mae: 1.5505\n",
            "Epoch 83/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.5031 - mae: 1.5218\n",
            "Epoch 84/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.4680 - mae: 1.4954\n",
            "Epoch 85/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.4062 - mae: 1.4892\n",
            "Epoch 86/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.4239 - mae: 1.4957\n",
            "Epoch 87/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.3409 - mae: 1.4847\n",
            "Epoch 88/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.3253 - mae: 1.4796\n",
            "Epoch 89/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.2964 - mae: 1.4782\n",
            "Epoch 90/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.2640 - mae: 1.4643\n",
            "Epoch 91/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.3203 - mae: 1.4709\n",
            "Epoch 92/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.2941 - mae: 1.4790\n",
            "Epoch 93/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.2220 - mae: 1.4691\n",
            "Epoch 94/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.2463 - mae: 1.4568\n",
            "Epoch 95/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.3799 - mae: 1.4925\n",
            "Epoch 96/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.2625 - mae: 1.4728\n",
            "Epoch 97/200\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 4.2968 - mae: 1.4902\n",
            "Epoch 98/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.1631 - mae: 1.4512\n",
            "Epoch 99/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.1606 - mae: 1.4538\n",
            "Epoch 100/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.1170 - mae: 1.4436\n",
            "Epoch 101/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.1475 - mae: 1.4436\n",
            "Epoch 102/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.1019 - mae: 1.4365\n",
            "Epoch 103/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.0864 - mae: 1.4378\n",
            "Epoch 104/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.2164 - mae: 1.4538\n",
            "Epoch 105/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.0888 - mae: 1.4458\n",
            "Epoch 106/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.0518 - mae: 1.4373\n",
            "Epoch 107/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.0124 - mae: 1.4279\n",
            "Epoch 108/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.9665 - mae: 1.4179\n",
            "Epoch 109/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.9357 - mae: 1.4093\n",
            "Epoch 110/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.9871 - mae: 1.4301\n",
            "Epoch 111/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.0966 - mae: 1.4508\n",
            "Epoch 112/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.9861 - mae: 1.4254\n",
            "Epoch 113/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.1018 - mae: 1.4681\n",
            "Epoch 114/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.9518 - mae: 1.4224\n",
            "Epoch 115/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.0522 - mae: 1.4363\n",
            "Epoch 116/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.9666 - mae: 1.4359\n",
            "Epoch 117/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.9320 - mae: 1.4312\n",
            "Epoch 118/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.0476 - mae: 1.4442\n",
            "Epoch 119/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.8028 - mae: 1.3955\n",
            "Epoch 120/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.8459 - mae: 1.4010\n",
            "Epoch 121/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.7679 - mae: 1.3846\n",
            "Epoch 122/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.7653 - mae: 1.3794\n",
            "Epoch 123/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.7936 - mae: 1.3855\n",
            "Epoch 124/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.6808 - mae: 1.3640\n",
            "Epoch 125/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.7193 - mae: 1.3715\n",
            "Epoch 126/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.7967 - mae: 1.3879\n",
            "Epoch 127/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.7058 - mae: 1.3743\n",
            "Epoch 128/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.7658 - mae: 1.3775\n",
            "Epoch 129/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.7040 - mae: 1.3659\n",
            "Epoch 130/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.6880 - mae: 1.3631\n",
            "Epoch 131/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.6753 - mae: 1.3679\n",
            "Epoch 132/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.6640 - mae: 1.3626\n",
            "Epoch 133/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.5888 - mae: 1.3435\n",
            "Epoch 134/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.5797 - mae: 1.3532\n",
            "Epoch 135/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.5943 - mae: 1.3568\n",
            "Epoch 136/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.5807 - mae: 1.3460\n",
            "Epoch 137/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.6539 - mae: 1.3857\n",
            "Epoch 138/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.5035 - mae: 1.3410\n",
            "Epoch 139/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.5833 - mae: 1.3478\n",
            "Epoch 140/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.5077 - mae: 1.3438\n",
            "Epoch 141/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.5091 - mae: 1.3301\n",
            "Epoch 142/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.5569 - mae: 1.3344\n",
            "Epoch 143/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.4735 - mae: 1.3463\n",
            "Epoch 144/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.4644 - mae: 1.3404\n",
            "Epoch 145/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.4190 - mae: 1.3256\n",
            "Epoch 146/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.5012 - mae: 1.3433\n",
            "Epoch 147/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3859 - mae: 1.3137\n",
            "Epoch 148/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3904 - mae: 1.3043\n",
            "Epoch 149/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3870 - mae: 1.3221\n",
            "Epoch 150/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3857 - mae: 1.3179\n",
            "Epoch 151/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.4297 - mae: 1.3380\n",
            "Epoch 152/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3795 - mae: 1.3177\n",
            "Epoch 153/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3162 - mae: 1.2905\n",
            "Epoch 154/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3686 - mae: 1.3158\n",
            "Epoch 155/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3212 - mae: 1.3127\n",
            "Epoch 156/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3338 - mae: 1.3041\n",
            "Epoch 157/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3319 - mae: 1.3128\n",
            "Epoch 158/200\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 3.2819 - mae: 1.3015\n",
            "Epoch 159/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3282 - mae: 1.3051\n",
            "Epoch 160/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.2951 - mae: 1.2979\n",
            "Epoch 161/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3644 - mae: 1.2967\n",
            "Epoch 162/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.3146 - mae: 1.2988\n",
            "Epoch 163/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.2543 - mae: 1.2987\n",
            "Epoch 164/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.2883 - mae: 1.3048\n",
            "Epoch 165/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.2366 - mae: 1.2949\n",
            "Epoch 166/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.2306 - mae: 1.2802\n",
            "Epoch 167/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.2160 - mae: 1.2980\n",
            "Epoch 168/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.1582 - mae: 1.2771\n",
            "Epoch 169/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.2138 - mae: 1.2920\n",
            "Epoch 170/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.2209 - mae: 1.3021\n",
            "Epoch 171/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.1695 - mae: 1.2883\n",
            "Epoch 172/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.1416 - mae: 1.2844\n",
            "Epoch 173/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.1334 - mae: 1.2747\n",
            "Epoch 174/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.1307 - mae: 1.2665\n",
            "Epoch 175/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.0858 - mae: 1.2661\n",
            "Epoch 176/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.0723 - mae: 1.2557\n",
            "Epoch 177/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.1182 - mae: 1.2783\n",
            "Epoch 178/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.1100 - mae: 1.2578\n",
            "Epoch 179/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.1168 - mae: 1.2747\n",
            "Epoch 180/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.0372 - mae: 1.2675\n",
            "Epoch 181/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.0597 - mae: 1.2541\n",
            "Epoch 182/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.0360 - mae: 1.2491\n",
            "Epoch 183/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.0457 - mae: 1.2567\n",
            "Epoch 184/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.1508 - mae: 1.2771\n",
            "Epoch 185/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.0638 - mae: 1.2761\n",
            "Epoch 186/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.0096 - mae: 1.2487\n",
            "Epoch 187/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.9586 - mae: 1.2358\n",
            "Epoch 188/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.0368 - mae: 1.2584\n",
            "Epoch 189/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.9166 - mae: 1.2357\n",
            "Epoch 190/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.9291 - mae: 1.2521\n",
            "Epoch 191/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.1694 - mae: 1.2702\n",
            "Epoch 192/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.9129 - mae: 1.2398\n",
            "Epoch 193/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.8819 - mae: 1.2289\n",
            "Epoch 194/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.8860 - mae: 1.2338\n",
            "Epoch 195/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.9044 - mae: 1.2260\n",
            "Epoch 196/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.9164 - mae: 1.2392\n",
            "Epoch 197/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.8533 - mae: 1.2078\n",
            "Epoch 198/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.8710 - mae: 1.2321\n",
            "Epoch 199/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.8813 - mae: 1.2345\n",
            "Epoch 200/200\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.9126 - mae: 1.2285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test_scaled, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2TDU7_IhdbL",
        "outputId": "b74777ce-f648-4b10-d4b5-8ce44bbe7d17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 15.6887 - mae: 2.6884\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15.688724517822266, 2.6884026527404785]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history['mae'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMpH6jwBhLBw",
        "outputId": "d15270a4-21d4-41a5-e51f-729f1ef0a9ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.741974949836731, 1.7431538105010986, 1.6963014602661133, 1.7049437761306763, 1.715605616569519, 1.7188698053359985, 1.693077802658081, 1.6964553594589233, 1.6934174299240112, 1.694053292274475, 1.6990057229995728, 1.6926467418670654, 1.6775867938995361, 1.6904059648513794, 1.6683886051177979, 1.6857787370681763, 1.6865688562393188, 1.6588151454925537, 1.6667405366897583, 1.6863164901733398, 1.6523640155792236, 1.6750407218933105, 1.6611987352371216, 1.6602299213409424, 1.6866891384124756, 1.6578062772750854, 1.6558468341827393, 1.6590845584869385, 1.6505223512649536, 1.6632016897201538, 1.656231164932251, 1.6365538835525513, 1.621346354484558, 1.6651437282562256, 1.6479533910751343, 1.6235287189483643, 1.6111574172973633, 1.6238986253738403, 1.615957260131836, 1.5952646732330322, 1.6040480136871338, 1.6346684694290161, 1.6032313108444214, 1.6176759004592896, 1.623969554901123, 1.5994240045547485, 1.59097421169281, 1.6101112365722656, 1.5970394611358643, 1.576608657836914, 1.5794316530227661, 1.5616146326065063, 1.5761888027191162, 1.5548478364944458, 1.6008516550064087, 1.5689817667007446, 1.5511819124221802, 1.5507969856262207, 1.581574559211731, 1.5625859498977661, 1.5480597019195557, 1.5503864288330078, 1.5350631475448608, 1.5550920963287354, 1.536056399345398, 1.5683118104934692, 1.5244331359863281, 1.5151947736740112, 1.514891266822815, 1.5359441041946411, 1.5132688283920288, 1.5123077630996704, 1.5069568157196045, 1.5062092542648315, 1.5057514905929565, 1.4958865642547607, 1.4935630559921265, 1.493167519569397, 1.4872214794158936, 1.5238951444625854, 1.5306408405303955, 1.550529956817627, 1.5218015909194946, 1.4953505992889404, 1.4892295598983765, 1.4957433938980103, 1.4846930503845215, 1.4796074628829956, 1.4782313108444214, 1.464261770248413, 1.4708830118179321, 1.4789944887161255, 1.4691139459609985, 1.4567617177963257, 1.492531180381775, 1.4728388786315918, 1.4902034997940063, 1.4511995315551758, 1.4538166522979736, 1.4435771703720093, 1.443600058555603, 1.4364551305770874, 1.4377881288528442, 1.453775405883789, 1.4458189010620117, 1.4373263120651245, 1.4278898239135742, 1.4179149866104126, 1.4093180894851685, 1.4301064014434814, 1.4508461952209473, 1.4254426956176758, 1.468071460723877, 1.4223741292953491, 1.436277985572815, 1.4359363317489624, 1.431183934211731, 1.4442166090011597, 1.3955247402191162, 1.4010274410247803, 1.3845906257629395, 1.3793957233428955, 1.3854814767837524, 1.3639898300170898, 1.3715285062789917, 1.3878560066223145, 1.3742663860321045, 1.377453088760376, 1.3658753633499146, 1.3631314039230347, 1.3679096698760986, 1.3626233339309692, 1.3435267210006714, 1.3532241582870483, 1.3567650318145752, 1.3460288047790527, 1.3856613636016846, 1.3409932851791382, 1.3478116989135742, 1.3438010215759277, 1.3301347494125366, 1.3343781232833862, 1.3463202714920044, 1.3403619527816772, 1.325621247291565, 1.3433374166488647, 1.3136835098266602, 1.3043031692504883, 1.3221453428268433, 1.317914605140686, 1.338013768196106, 1.317684531211853, 1.290511965751648, 1.3157720565795898, 1.3127158880233765, 1.3041210174560547, 1.3127871751785278, 1.3015096187591553, 1.3050549030303955, 1.297927975654602, 1.2967067956924438, 1.298798680305481, 1.298659324645996, 1.3048162460327148, 1.2949190139770508, 1.2801990509033203, 1.2979810237884521, 1.2770661115646362, 1.2919821739196777, 1.3020931482315063, 1.2882746458053589, 1.284400224685669, 1.2747364044189453, 1.2665088176727295, 1.2660579681396484, 1.2557158470153809, 1.2782955169677734, 1.2577747106552124, 1.2746994495391846, 1.2674518823623657, 1.2540546655654907, 1.2490705251693726, 1.25667405128479, 1.2771363258361816, 1.2760717868804932, 1.2486706972122192, 1.2358362674713135, 1.2583509683609009, 1.2357438802719116, 1.252081274986267, 1.2701975107192993, 1.2398251295089722, 1.2288800477981567, 1.2338227033615112, 1.225972294807434, 1.2391548156738281, 1.2078160047531128, 1.2321351766586304, 1.234458088874817, 1.2284926176071167]\n"
          ]
        }
      ]
    }
  ]
}