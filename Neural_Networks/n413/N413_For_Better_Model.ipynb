{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N413_For_Better_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jub-GSrlhpo"
      },
      "source": [
        "<img src='https://user-images.githubusercontent.com/6457691/90080969-0f758d00-dd47-11ea-8191-fa12fd2054a7.png' width = '200' align = 'right'>\n",
        "\n",
        "## *AIB / SECTION 4 / SPRINT 1 / NOTE 3*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Floqm6KJlkZw"
      },
      "source": [
        "# N413. 더 나은 신경망 학습을 위한 방법들 - 학습률(Learning rate), 가중치 초기화(Weight Initialization) & Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 더 좋은 결과를 만들어봅시다!"
      ],
      "metadata": {
        "id": "mtH1CJu43USP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "지난 강의를 통해 신경망의 기본 구조가 되는 퍼셉트론과 신경망 학습에 대해 이해하고,<br/>\n",
        "Keras를 사용하여 기본적인 신경망을 구축해 보았습니다.\n",
        "\n",
        "이번 강의에서는 신경망 학습이 더 잘되도록 하는 방법에 대해 알아보도록 하겠습니다."
      ],
      "metadata": {
        "id": "1z-77Jd4mYMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. 학습률 감소/계획법(Learning rate Decay/Scheduling)"
      ],
      "metadata": {
        "id": "xW3K5omLxZQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫 번째로 알아볼 기법은 **학습률 감소 및 계획법(Learning rate Decay/Scheduling)** 입니다.<br/>\n",
        "위 방법에 대해 알아보기 전에 먼저 학습률에 대해 알아보도록 하겠습니다."
      ],
      "metadata": {
        "id": "QfQTMibdZq2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **학습률(Learning rate)**\n",
        "\n",
        "**<font color=\"ff6f61\">학습률(Learning rate, `lr`)</font>**이란 매 가중치에 대해 구해진 기울기 값을 얼마나 경사 하강법에 적용할 지를 결정하는 하이퍼파라미터입니다.<br/>\n",
        "지난 시간에 보았던 경사 하강법 수식에서 학습률이 어디에 위치하는지 확인해봅시다."
      ],
      "metadata": {
        "id": "fqd1TOUv6102"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/ic91umJ.png\" height=\"200\"/>"
      ],
      "metadata": {
        "id": "3AVX33ZINayy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 식에서 볼 수 있는 것처럼 해당 지점에서의 기울기를 구하여 기울기가 감소하는$(-)$ 방향으로 이동하게 되는데요.<br/>\n",
        "학습률은 **얼마나 이동할 지를 조정하는 하이퍼파라미터**입니다.\n",
        "\n",
        "경사 하강법이 산길을 내려가는 과정이라면 학습률은 **보폭을 결정**하게 됩니다.<br/>\n",
        "학습률이 크면 보폭이 크니 Iteration 마다 성큼성큼 이동하고, 작으면 보폭이 작아 조금씩만 이동하게 됩니다.<br/>\n",
        "그렇다면 학습률을 잘못 설정하면 어떻게 될까요?"
      ],
      "metadata": {
        "id": "TCZyAvXYNeBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **학습률이 너무 크거나 작으면 어떻게 될까요?**\n",
        "\n",
        "> ❗️ ***아래는 학습률이 너무 클 때와 작을 때의 경사하강법을 나타낸 그림입니다.<br/>\n",
        "그림을 기억하면서 최적의 학습률이 왜 중요한 지에 대해 생각해봅시다.***"
      ],
      "metadata": {
        "id": "HbBjzju8N1zo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/RfBFgKs.png\" height=\"300\">"
      ],
      "metadata": {
        "id": "3JJV693y7fUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그림에서 확인할 수 있는 것처럼<br/>\n",
        "**학습률이 너무 낮으면 최적점에 이르기까지 너무 오래 걸리거나, 주어진 Iteration 내에서 최적점에 도달하는 데 실패**하기도 합니다.<br/>\n",
        "반대로 **너무 높으면 경사 하강 과정에서 발산하면서 모델이 최적값을 찾을 수 없게** 되어버립니다.\n",
        "\n",
        "그렇기 때문에 최적의 학습률을 찾는 것은 학습에서 중요한 요소인데요.<br/>\n",
        "위와 같은 문제를 해결하기 위해서 사용되는 방법이 **학습률 감소/계획법**입니다.\n"
      ],
      "metadata": {
        "id": "s-hKd9jMK6xE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **학습률 감소/계획법(Learning rate Decay/Scheduling)**\n",
        "<br/>\n",
        "\n",
        "1. **학습률 감소(Learning rate Decay)**\n",
        "\n",
        "    학습률 감소는 Adagrad, RMSprop, Adam 과 같은 주요 옵티마이저에 이미 구현되어 있기 때문에 쉽게 적용할 수 있습니다.<br/>\n",
        "    해당 옵티마티저의 하이퍼파라미터를 조정하면 감소 정도를 변화시킬 수 있습니다.\n",
        "\n",
        "    아래와 같이 **`.compile`** 내에 있는 **`optimizer=`** 에 Adam 등의 옵티마이저 적용 후<br/> 내부 하이퍼파라미터를 변경하면 학습률 감소를 적용할 수 있습니다."
      ],
      "metadata": {
        "id": "ONqTmzH6cBTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1 = 0.89)\n",
        "             , loss='sparse_categorical_crossentropy'\n",
        "             , metrics=['accuracy'])\n",
        "```"
      ],
      "metadata": {
        "id": "v3MeupB-3I8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **학습률 계획법(Learning rate Scheduling)**<br/>\n",
        "\n",
        "    아래 그래프와 같이 Warm-up Step 을 포함한 학습률 계획 방법을 적용하기도 합니다.\n",
        "    \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "oMGFygrreCCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/Ida3BHl.png\" height=\"200\"/>\n"
      ],
      "metadata": {
        "id": "6h5f6oUPLysS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습률 계획은 다음과 같이 **`.experimental`** 내부의 함수를 사용하여 설계할 수 있습니다."
      ],
      "metadata": {
        "id": "Za3bNn1WNTps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "first_decay_steps = 1000\n",
        "initial_learning_rate = 0.01\n",
        "lr_decayed_fn = (\n",
        "  tf.keras.experimental.CosineDecayRestarts(\n",
        "      initial_learning_rate,\n",
        "      first_decay_steps))\n",
        "```\n",
        "\n",
        "```python\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_decayed_fn)\n",
        "             , loss='sparse_categorical_crossentropy'\n",
        "             , metrics=['accuracy'])\n",
        "```"
      ],
      "metadata": {
        "id": "0KdjVCv1ys_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. 가중치 초기화(Weight Initialization)"
      ],
      "metadata": {
        "id": "ITXsFgNr3Zvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "초기 가중치 설정과 관련된 **<font color=\"ff6f61\">가중치 초기화(Weight initialization)</font>**는 신경망에서 매우 중요한 요소입니다.\n",
        "\n",
        "아래에서는 먼저 가중치를 정규분포로 초기화하였을 때의 문제를 알아보고 <br/>\n",
        "흔히 사용되는 **Xavier 초기화**와 **He 초기화**에 대해 알아보도록 하겠습니다.\n",
        "\n",
        "> ❗️ ***아래 가중치 초기화 방법의 수식을 외울 필요는 없습니다.<br/>\n",
        "일단은 어떤 초기화 방법이 있고 해당 초기화 방법이 언제 사용되는지만 기억하고 넘어갑시다.***<br/>\n",
        "❗️ ***실제로는 더 많은 초기화 방법이 있습니다. 모든 초기화 방법을 기억할 필요는 없습니다.***"
      ],
      "metadata": {
        "id": "6YTPJ9LfG-ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **표준편차를 1인 정규분포로 가중치를 초기화 할 때 각 층의 활성화 값 분포**\n",
        "\n",
        "표준편차가 일정한 정규분포로 가중치를 초기화 해 줄 때에는 대부분의 활성화 값이 0과 1에 위치하는 것을 볼 수 있습니다.<br/>\n",
        "활성값이 고르지 못할 경우 학습이 제대로 이루어지지 않는데요.<br/>\n",
        "그렇기 때문에 가장 간단한 방법임에도 잘 사용되지 않습니다."
      ],
      "metadata": {
        "id": "c1fcHhNa5P7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/Zd6OsJT.png\" height=300/>"
      ],
      "metadata": {
        "id": "HqqgQoje5QkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Xavier 초기화를 해주었을 때의 활성화 값의 분포**\n",
        "\n",
        "**Xavier 초기화(Xavier initialization)**는 가중치를 표준편차가 고정값인 정규분포로 초기화 했을 때의 문제점을 해결하기 위해 등장한 방법입니다.<br/>\n",
        "Xavier 초기화는 이전 층의 노드가 $n$ 개일 때, 현재 층의 가중치를 표준편차가 $\\frac{1}{\\sqrt{n}}$ 인 정규분포로 초기화합니다.<br/>\n",
        "*(케라스에서 Xavier 초기화는 이전 층의 노드가 $n$ 개이고 현재 층의 노드가 $m$ 개일 때, 현재 층의 가중치를 표준편차가 $\\frac{2}{\\sqrt{n+m}}$ 인 정규분포로 초기화합니다.)*"
      ],
      "metadata": {
        "id": "HIjkSBAv5yOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/NXziIT2.png\" height=\"300\"/>\n"
      ],
      "metadata": {
        "id": "5Ptf1IeH54WW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **He 초기화를 해주었을 때의 활성화 값의 분포**\n",
        "\n",
        "Xavier 초기화는 활성화 함수가 시그모이드(`Sigmoid`)인 신경망에서는 잘 동작합니다.<br/>\n",
        "하지만 활성화 함수가 ReLU 일 경우에는 층이 지날수록 활성값이 고르지 못하게 되는 문제를 보이는데요.\n",
        "\n",
        "이런 문제를 해결하기 위해 등장한 것이 바로 **He 초기화(He initialization)** 입니다.<br/>\n",
        "He 초기화는 이전 층의 노드가 $n$ 개일 때, 현재 층의 가중치를 표준편차가 $\\frac{2}{\\sqrt{n}}$ 인 정규분포로 초기화합니다.<br/>\n",
        "He 초기화를 적용하면 아래 그림처럼 층이 지나도 활성값이 고르게 유지되는 것을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "P50TF7Q15412"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/gZRdLiM.png\" height=\"300\"/>"
      ],
      "metadata": {
        "id": "4FQq4UP855K8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "가중치 초기화 방법을 요약하면 다음과 같습니다.<br/>\n",
        "\n",
        "> **Activation function에 따른 초기값 추천**\n",
        "1. Sigmoid  ⇒  Xavier 초기화를 사용하는 것이 유리 \n",
        "2. ReLU  ⇒  He 초기화 사용하는 것이 유리"
      ],
      "metadata": {
        "id": "6QrgIw0x6Jss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "가중치 초기화 방법은 이외에도 여러 가지가 있습니다.<br/>\n",
        "케라스에서는 아래와 같은 가중치 초기화 방법을 제공하고 있습니다.\n",
        "\n",
        "> ❗️ ***모든 가중치 초기화 방법을 알 필요는 없습니다.***\n",
        "\n",
        "```python\n",
        "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "```\n"
      ],
      "metadata": {
        "id": "UCGTDCO-GYFr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llrI13kKPPDS"
      },
      "source": [
        "## 머신러닝으로서의 딥러닝"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래와 비슷한 그림을 보신 적이 있으신가요?"
      ],
      "metadata": {
        "id": "Z9J495H8zYOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/1MMrBS5.png\" height = \"200\"/>\n"
      ],
      "metadata": {
        "id": "5yiNfgIjlaV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 그림은 **인공지능(Artificial Intelligence)**과 **머신러닝(Machine Learning)** 및 **딥러닝(Deep Learning)**의 관계를 나타내고 있습니다.\n",
        "\n",
        "그림에서 볼 수 있는 것처럼 Section 4 에서 다루고 있는 딥러닝, 즉 인공 신경망도 역시 머신러닝 모델이라고 할 수 있는데요.<br/>\n",
        "그렇기 때문에 머신러닝 모델에서 발생할 수 있는 문제가 동일하게 발생하고, 문제를 해결할 수 있는 방법 역시 동일하게 적용할 수 있습니다.\n"
      ],
      "metadata": {
        "id": "Pk9Z4QuTlXmH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br8Or5nHyEqY"
      },
      "source": [
        "### 과적합(Overfitting)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZcKMjDcIaEZ"
      },
      "source": [
        "인공 신경망의 노드 수와 층을 늘리다 보면 매개 변수가 상당히 많아집니다.\n",
        "\n",
        "Fashion MNIST 예제를 풀기 위해서 구축한 신경망에서는 은닉층 없이 출력층만 설계했음에도 7,850개의 파라미터가 있었는데요.<br/>\n",
        "딥러닝, 즉 은닉층이 3개 이상인 신경망에는 훨씬 더 많은 수의 파라미터가 있습니다.\n",
        "\n",
        "머신러닝에서는 모델이 복잡해지면 **<font color=\"ff6f61\">과적합(Overfitting)</font>** 문제가 발생하는 경향이 있는데요.<br/>\n",
        "이번 강의에서는 과적합 방지를 위해 적용하는 방법에 대해서 알아보겠습니다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 과적합 방지를 위한 방법들"
      ],
      "metadata": {
        "id": "XVLzxrorw5lq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn7cs2ym48bJ"
      },
      "source": [
        "#### 1. Weight Decay (가중치 감소)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫 번째로 알아볼 방법은 **<font color=\"ff6f61\">가중치 감소(Weight Decay)</font>** 입니다.\n",
        "\n",
        "과적합은 가중치의 값이 클 때 주로 발생하는데요.<br/>\n",
        "가중치 감소에서는 가중치가 너무 커지지 않도록 **가중치 값이 너무 커지지 않도록 조건을 추가**합니다.<br/>\n",
        "이 과정에서 **손실 함수(Cost function)에 가중치와 관련된 항을 추가**됩니다.<br/>\n",
        "조건을 어떻게 적용할지에 따라 L1 Regularization(LASSO), L2 Regularization(Ridge) 으로 나뉩니다.\n",
        "\n",
        "각 방법에 따른 손실 함수 식은 다음과 같습니다.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "L_1(\\theta_w) &= \\frac{1}{2} \\sum_i (output_i - target_i)^2 + \\color{blue}{\\lambda} \\cdot \\color{red}{\\Vert \\theta_w \\Vert_1}\\\\\n",
        "L_2(\\theta_w) &= \\frac{1}{2} \\sum_i (output_i - target_i)^2 + \\color{blue}{\\lambda} \\cdot \\color{red}{\\Vert \\theta_w \\Vert_2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Keras 에서는 아래와 같이 가중치 감소를 적용하고 싶은 층에 `regularizer` 파라미터를 추가하면 됩니다.\n",
        "\n",
        "> ❗️ ***`kernel_regularizer` 와 `activity_regularizer` 의 차이에 대해서는 일단 넘어갑시다.***"
      ],
      "metadata": {
        "id": "yKKDlCM5uNtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "Dense(64,\n",
        "      kernel_regularizer=regularizers.l2(0.01),\n",
        "      activity_regularizer=regularizers.l1(0.01))\n",
        "```"
      ],
      "metadata": {
        "id": "_ffPiuNWdljb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Dropout (드롭아웃)"
      ],
      "metadata": {
        "id": "8SQsF4emxP1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropout(드롭아웃)**은 Iteration 마다 **레이어 노드 중 일부를 사용하지 않으면서 학습을 진행하는 방법**입니다.<br/>\n",
        "매 번 다른 노드가 학습되면서 전체 가중치가 과적합되는 것을 방지할 수 있습니다.<br/>\n",
        "아래는 Dropout을 적용했을 때의 신경망에서 학습되는 노드를 나타낸 그림입니다."
      ],
      "metadata": {
        "id": "vWndbcDy20UL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/rAyIZxV.png\" height=\"300\">"
      ],
      "metadata": {
        "id": "H5M7ArjZG-lZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout 을 적용할 때에는 0~1 사이의 실수를 입력하게 되는데요.<br/>\n",
        "모델 내에 있는 특정 레이어의 노드 연결을 지정해 준 비율만큼 강제로 끊습니다.<br/>\n",
        "매 Iteration 마다 랜덤하게 노드를 차단하여 다른 가중치를 학습하도록 조정하기 때문에 과적합을 방지할 수 있게 됩니다.\n",
        "\n",
        "Keras 에서는 아래와 같이 Dropout 을 적용하고 싶은 층 다음에 `Dropout` 함수를 추가하면 됩니다."
      ],
      "metadata": {
        "id": "RnxtUB2kHHqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "Dense(64,\n",
        "      kernel_regularizer=regularizers.l2(0.01),\n",
        "      activity_regularizer=regularizers.l1(0.01))\n",
        "Dropout(0.5)\n",
        "```"
      ],
      "metadata": {
        "id": "MpfD4gXTdt6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Early Stopping (조기 종료)\n"
      ],
      "metadata": {
        "id": "SAvwmpOFxJbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫 번째 방법은 **<font color=\"ff6f61\">조기 종료(Early Stopping)</font>**입니다.<br/>\n",
        "아래 그림에서 볼 수 있는 것처럼 **학습(Train) 데이터에 대한 손실은 계속 줄어들지만<br/>\n",
        "검증(Validation) 데이터셋에 대한 손실은 증가한다면 학습을 종료**하도록 설정하는 방법입니다."
      ],
      "metadata": {
        "id": "JSUZI83IzgfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/gTdMIuD.png\" height=\"300\">"
      ],
      "metadata": {
        "id": "Z5jjH7Rs1oxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 Fashion MNIST 예제에서 구축 신경망에 **조기 종료(Early Stopping)**를 적용하여 보겠습니다."
      ],
      "metadata": {
        "id": "OMFMR4922guk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fashion MNIST 신경망 예제에 기법 적용하기"
      ],
      "metadata": {
        "id": "eeq9Fkhm3BlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "metadata": {
        "id": "pf415x9HeuHJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "k-shEO-j7YVC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **데이터셋을 불러옵니다.**"
      ],
      "metadata": {
        "id": "yuHlAVpS4ix1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXzVrPl3jZSL",
        "outputId": "b2f11b3d-db86-42f4-dd39-1df3aea8658e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **데이터를 정규화(Normalization)합니다.**"
      ],
      "metadata": {
        "id": "v8ETLEjs4k5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255.\n",
        "X_test = X_test / 255."
      ],
      "metadata": {
        "id": "VGkSj-XVjdOP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **레이블의 개수와 형태를 확인합니다.**"
      ],
      "metadata": {
        "id": "TORaDD1U4uMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF-QSeSLjdp0",
        "outputId": "6d5e7387-f097-4fd6-e883-0d14b6427dc7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **신경망 모델을 구축하고 Compile 합니다.**\n",
        "\n",
        "구축 과정에서 위에서 학습하였던 **Weight Decay(가중치 감소), Dropout(드롭아웃)**을 적용해봅시다."
      ],
      "metadata": {
        "id": "wCC0Z2ZH49ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본적인 신경망을 만드는 코드\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(64,\n",
        "          kernel_regularizer=regularizers.l2(0.01),\n",
        "          activity_regularizer=regularizers.l1(0.01)),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "iZBg5E4ZjSrQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile 설정에서 위에서 학습하였던 **Learning rate Decay(학습률 감소)**를 적용해봅니다."
      ],
      "metadata": {
        "id": "IzSIlxcx5SBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1 = 0.89)\n",
        "             , loss='sparse_categorical_crossentropy'\n",
        "             , metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "TkJM3eSR3ue9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFqWpUI1jmrS",
        "outputId": "9d56bce3-adbc-4e30-9869-964f87271f69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                50240     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50,890\n",
            "Trainable params: 50,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **신경망 모델을 학습합니다.**\n",
        "\n",
        "먼저 학습 과정에서 **Early Stopping(조기 종료)**를 적용할 수 있도록<br/>\n",
        "파라미터 저장 경로와 조기 종료 옵션을 설정하여 줍니다."
      ],
      "metadata": {
        "id": "wpno3z7S5ba1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파라미터 저장 경로를 설정하는 코드입니다.\n",
        "checkpoint_filepath = \"FMbest.hdf5\"\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)"
      ],
      "metadata": {
        "id": "hsnPUwiS50_Y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_best = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
        "    save_weights_only=True, mode='auto', save_freq='epoch', options=None)"
      ],
      "metadata": {
        "id": "EDVLFetx0bLX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, batch_size=32, epochs=30, verbose=1, \n",
        "          validation_data=(X_test,y_test), \n",
        "          callbacks=[early_stop, save_best])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yCFF-P8kwym",
        "outputId": "ced09f7a-24b0-413d-c451-6599ad0de9c3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1855/1875 [============================>.] - ETA: 0s - loss: 1.2151 - accuracy: 0.7680\n",
            "Epoch 00001: val_loss improved from inf to 0.86058, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 8s 3ms/step - loss: 1.2122 - accuracy: 0.7682 - val_loss: 0.8606 - val_accuracy: 0.8136\n",
            "Epoch 2/30\n",
            "1857/1875 [============================>.] - ETA: 0s - loss: 0.9307 - accuracy: 0.7883\n",
            "Epoch 00002: val_loss did not improve from 0.86058\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.9306 - accuracy: 0.7884 - val_loss: 0.8889 - val_accuracy: 0.7957\n",
            "Epoch 3/30\n",
            "1853/1875 [============================>.] - ETA: 0s - loss: 0.9067 - accuracy: 0.7920\n",
            "Epoch 00003: val_loss improved from 0.86058 to 0.85553, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.9084 - accuracy: 0.7915 - val_loss: 0.8555 - val_accuracy: 0.7969\n",
            "Epoch 4/30\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.8966 - accuracy: 0.7914\n",
            "Epoch 00004: val_loss improved from 0.85553 to 0.85230, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8966 - accuracy: 0.7914 - val_loss: 0.8523 - val_accuracy: 0.8048\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.8916 - accuracy: 0.7940\n",
            "Epoch 00005: val_loss improved from 0.85230 to 0.81764, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8916 - accuracy: 0.7940 - val_loss: 0.8176 - val_accuracy: 0.8076\n",
            "Epoch 6/30\n",
            "1864/1875 [============================>.] - ETA: 0s - loss: 0.8902 - accuracy: 0.7917\n",
            "Epoch 00006: val_loss improved from 0.81764 to 0.80993, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.8903 - accuracy: 0.7916 - val_loss: 0.8099 - val_accuracy: 0.8103\n",
            "Epoch 7/30\n",
            "1857/1875 [============================>.] - ETA: 0s - loss: 0.8902 - accuracy: 0.7906\n",
            "Epoch 00007: val_loss did not improve from 0.80993\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8900 - accuracy: 0.7908 - val_loss: 0.8834 - val_accuracy: 0.7917\n",
            "Epoch 8/30\n",
            "1858/1875 [============================>.] - ETA: 0s - loss: 0.8866 - accuracy: 0.7928\n",
            "Epoch 00008: val_loss did not improve from 0.80993\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.8867 - accuracy: 0.7927 - val_loss: 0.8359 - val_accuracy: 0.8089\n",
            "Epoch 9/30\n",
            "1859/1875 [============================>.] - ETA: 0s - loss: 0.8826 - accuracy: 0.7922\n",
            "Epoch 00009: val_loss did not improve from 0.80993\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8825 - accuracy: 0.7920 - val_loss: 0.8158 - val_accuracy: 0.8079\n",
            "Epoch 10/30\n",
            "1861/1875 [============================>.] - ETA: 0s - loss: 0.8824 - accuracy: 0.7936\n",
            "Epoch 00010: val_loss did not improve from 0.80993\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8828 - accuracy: 0.7935 - val_loss: 0.8914 - val_accuracy: 0.7905\n",
            "Epoch 11/30\n",
            "1866/1875 [============================>.] - ETA: 0s - loss: 0.8818 - accuracy: 0.7931\n",
            "Epoch 00011: val_loss did not improve from 0.80993\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8815 - accuracy: 0.7931 - val_loss: 0.8484 - val_accuracy: 0.7904\n",
            "Epoch 12/30\n",
            "1863/1875 [============================>.] - ETA: 0s - loss: 0.8824 - accuracy: 0.7948\n",
            "Epoch 00012: val_loss improved from 0.80993 to 0.80203, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8825 - accuracy: 0.7946 - val_loss: 0.8020 - val_accuracy: 0.8125\n",
            "Epoch 13/30\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.8830 - accuracy: 0.7925\n",
            "Epoch 00013: val_loss did not improve from 0.80203\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8827 - accuracy: 0.7926 - val_loss: 0.8037 - val_accuracy: 0.8177\n",
            "Epoch 14/30\n",
            "1866/1875 [============================>.] - ETA: 0s - loss: 0.8778 - accuracy: 0.7919\n",
            "Epoch 00014: val_loss improved from 0.80203 to 0.79871, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8778 - accuracy: 0.7919 - val_loss: 0.7987 - val_accuracy: 0.8091\n",
            "Epoch 15/30\n",
            "1863/1875 [============================>.] - ETA: 0s - loss: 0.8850 - accuracy: 0.7915\n",
            "Epoch 00015: val_loss did not improve from 0.79871\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8855 - accuracy: 0.7914 - val_loss: 0.8167 - val_accuracy: 0.8184\n",
            "Epoch 16/30\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.8901 - accuracy: 0.7909\n",
            "Epoch 00016: val_loss improved from 0.79871 to 0.78955, saving model to FMbest.hdf5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8900 - accuracy: 0.7908 - val_loss: 0.7895 - val_accuracy: 0.8196\n",
            "Epoch 17/30\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.8819 - accuracy: 0.7922\n",
            "Epoch 00017: val_loss did not improve from 0.78955\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8817 - accuracy: 0.7923 - val_loss: 0.8025 - val_accuracy: 0.8104\n",
            "Epoch 18/30\n",
            "1853/1875 [============================>.] - ETA: 0s - loss: 0.8839 - accuracy: 0.7911\n",
            "Epoch 00018: val_loss did not improve from 0.78955\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8834 - accuracy: 0.7913 - val_loss: 0.7941 - val_accuracy: 0.8178\n",
            "Epoch 19/30\n",
            "1856/1875 [============================>.] - ETA: 0s - loss: 0.8789 - accuracy: 0.7932\n",
            "Epoch 00019: val_loss did not improve from 0.78955\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8793 - accuracy: 0.7929 - val_loss: 0.8444 - val_accuracy: 0.8129\n",
            "Epoch 20/30\n",
            "1864/1875 [============================>.] - ETA: 0s - loss: 0.8757 - accuracy: 0.7930\n",
            "Epoch 00020: val_loss did not improve from 0.78955\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8755 - accuracy: 0.7930 - val_loss: 0.7939 - val_accuracy: 0.8219\n",
            "Epoch 21/30\n",
            "1864/1875 [============================>.] - ETA: 0s - loss: 0.8841 - accuracy: 0.7909\n",
            "Epoch 00021: val_loss did not improve from 0.78955\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.8842 - accuracy: 0.7909 - val_loss: 0.8484 - val_accuracy: 0.8004\n",
            "Epoch 22/30\n",
            "1855/1875 [============================>.] - ETA: 0s - loss: 0.8764 - accuracy: 0.7944\n",
            "Epoch 00022: val_loss did not improve from 0.78955\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8777 - accuracy: 0.7943 - val_loss: 0.8796 - val_accuracy: 0.7984\n",
            "Epoch 23/30\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 0.8776 - accuracy: 0.7934\n",
            "Epoch 00023: val_loss did not improve from 0.78955\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8772 - accuracy: 0.7936 - val_loss: 0.8187 - val_accuracy: 0.8187\n",
            "Epoch 24/30\n",
            "1862/1875 [============================>.] - ETA: 0s - loss: 0.8771 - accuracy: 0.7940\n",
            "Epoch 00024: val_loss did not improve from 0.78955\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8769 - accuracy: 0.7941 - val_loss: 0.7950 - val_accuracy: 0.8165\n",
            "Epoch 25/30\n",
            "1865/1875 [============================>.] - ETA: 0s - loss: 0.8773 - accuracy: 0.7926\n",
            "Epoch 00025: val_loss did not improve from 0.78955\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8781 - accuracy: 0.7924 - val_loss: 0.8525 - val_accuracy: 0.8029\n",
            "Epoch 26/30\n",
            "1865/1875 [============================>.] - ETA: 0s - loss: 0.8779 - accuracy: 0.7951\n",
            "Epoch 00026: val_loss did not improve from 0.78955\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8776 - accuracy: 0.7953 - val_loss: 0.8547 - val_accuracy: 0.8055\n",
            "Epoch 00026: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f68dc38c090>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **조기종료 직전의 모델을 사용하여 평가를 진행합니다.**"
      ],
      "metadata": {
        "id": "GMFmcflC03L7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test[0:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0uTr8Gd1UqF",
        "outputId": "d6aeaa45-7ca8-4d8b-844b-39f75ee22317"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.2075588e-05, 1.4129513e-06, 4.8530714e-05, 4.6155776e-05,\n",
              "        8.1829887e-05, 1.5290031e-01, 8.9300127e-05, 2.5638691e-01,\n",
              "        3.0169061e-03, 5.8740658e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTfCeDeOlDt9",
        "outputId": "10fa7fab-5e50-4b93-c0ff-ca14d1d82bab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.8547 - accuracy: 0.8055 - 504ms/epoch - 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **콜백(Callback)에 의해 Best 모델의 파라미터가 제대로 저장되었는지 확인하고 해당 모델로 평가를 진행합니다.**"
      ],
      "metadata": {
        "id": "T05UQlnPmufW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLPMWcTCmshO",
        "outputId": "84553aec-4409-4b12-db7e-32b769288897"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FMbest.hdf5  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "저장된 경로로부터 모델의 파라미터(가중치)를 불러옵니다."
      ],
      "metadata": {
        "id": "Kqg297g_1Ieh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(checkpoint_filepath)"
      ],
      "metadata": {
        "id": "16bPb8k3myJe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "불러온 모델을 사용하여 평가를 수행합니다."
      ],
      "metadata": {
        "id": "J_5td5zI1NZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test[0:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7KO7G4E1eFG",
        "outputId": "68477928-0685-4434-aefd-57b00f50b144"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.3812994e-05, 4.7637095e-05, 6.7187146e-05, 1.3367164e-04,\n",
              "        5.9769911e-05, 2.1045013e-01, 8.0051337e-05, 1.3290186e-01,\n",
              "        2.1713288e-03, 6.5405452e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXNs4GfWm0rI",
        "outputId": "efb707ec-a2ca-436d-97c2-53dfd8a5b4f7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.7895 - accuracy: 0.8196\n"
          ]
        }
      ]
    }
  ]
}